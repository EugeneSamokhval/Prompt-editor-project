{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30153c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "from os.path import isfile, join\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy\n",
    "from os import listdir\n",
    "import matplotlib.pyplot as plt\n",
    "from langdetect import detect, LangDetectException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abbe2d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english(text):\n",
    "    try:\n",
    "        if pd.isna(text) or str(text).strip() == \"\":\n",
    "            return False\n",
    "        return detect(str(text)) == 'en'\n",
    "    except LangDetectException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f57f9148",
   "metadata": {},
   "outputs": [],
   "source": [
    "midjourney_dataset_path = \"E:\\\\Diploma\\\\model_training\\\\datasets\\\\MidjourneyPrompts\"\n",
    "files_list = [f for f in listdir(midjourney_dataset_path) if isfile(join(midjourney_dataset_path, f))]\n",
    "\n",
    "midjourney_dataset = []\n",
    "\n",
    "# Collect text content from JSON files\n",
    "for json_file in files_list:\n",
    "    with open(join(midjourney_dataset_path, json_file), encoding=\"utf-8\") as current_file:\n",
    "        temp_json_representation = json.load(current_file)\n",
    "        for message_list in temp_json_representation.get(\"messages\", []):\n",
    "            for message in message_list:\n",
    "                midjourney_dataset.append(message.get(\"content\"))\n",
    "\n",
    "# Convert list of strings into a DataFrame\n",
    "df = pd.DataFrame(data=midjourney_dataset, columns=[\"content\"])\n",
    "\n",
    "# Extract text between < and >; create a new column\n",
    "df[\"extracted\"] = df[\"content\"].str.extract(r\">(.*?)<\")\n",
    "\n",
    "# Keep only rows where we found a match (non-NaN)\n",
    "df.dropna(subset=[\"extracted\"], inplace=True)\n",
    "\n",
    "# Remove exact duplicates before regex extraction\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(\"datasets/midjourney_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a209b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Diploma\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]

    }
   ],
   "source": [
    "# Download missing datasets\n",
    "df_SDXL_prompts = pd.read_parquet(\"hf://datasets/Falah/image_generation_prompts_SDXL/data/train-00000-of-00001-423acaf31a7beff6.parquet\")\n",
    "df_SDXL_prompts.to_csv(\"E:\\Diploma\\model_training\\datasets\\SDXL_prompts.csv\")\n",
    "\n",
    "df_midjourney_prompts_2 = pd.read_parquet(\"hf://datasets/Geonmo/midjourney-prompts-only/data/train-00000-of-00001-61a1e80026db4b04.parquet\")\n",
    "df_midjourney_prompts_2.to_csv(\"E:\\Diploma\\model_training\\datasets\\midjourney_prompts_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9236cbf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mParserError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m df_midjourney_prompts = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mE:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDiploma\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodel_training\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmidjourney_dataset.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m df_chat_gpt_prompts = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33mE:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDiploma\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodel_training\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mchatgpt_prompts.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m df_chat_gpt_4_prompts = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mE:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mDiploma\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mmodel_training\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mdatasets\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mchatgpt_dataset.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m df_SDXL_prompts = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mE:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDiploma\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodel_training\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mSDXL_prompts.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m df_midjourney_prompts_2 = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33mE:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDiploma\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmodel_training\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mdatasets\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mmidjourney_prompts_2.csv\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Diploma\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Diploma\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Diploma\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[39m, in \u001b[36mTextFileReader.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m   1916\u001b[39m nrows = validate_integer(\u001b[33m\"\u001b[39m\u001b[33mnrows\u001b[39m\u001b[33m\"\u001b[39m, nrows)\n\u001b[32m   1917\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1918\u001b[39m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[32m   1919\u001b[39m     (\n\u001b[32m   1920\u001b[39m         index,\n\u001b[32m   1921\u001b[39m         columns,\n\u001b[32m   1922\u001b[39m         col_dict,\n\u001b[32m-> \u001b[39m\u001b[32m1923\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[32m   1924\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[32m   1925\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1926\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m   1927\u001b[39m     \u001b[38;5;28mself\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Diploma\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[39m, in \u001b[36mCParserWrapper.read\u001b[39m\u001b[34m(self, nrows)\u001b[39m\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    233\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.low_memory:\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m         chunks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[32m    236\u001b[39m         data = _concatenate_chunks(chunks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:838\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader.read_low_memory\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:905\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._read_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:874\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:891\u001b[39m, in \u001b[36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mparsers.pyx:2061\u001b[39m, in \u001b[36mpandas._libs.parsers.raise_parser_error\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mParserError\u001b[39m: Error tokenizing data. C error: out of memory"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "#https://www.kaggle.com/datasets/tanreinama/900k-diffusion-prompts-dataset\n",
    "df_sd_prompts = pd.read_csv('E:\\Diploma\\model_training\\datasets\\SDPrompts\\diffusion_prompts.csv')\n",
    "\n",
    "#https://www.kaggle.com/datasets/succinctlyai/midjourney-texttoimage\n",
    "df_midjourney_prompts = pd.read_csv('E:\\Diploma\\model_training\\datasets\\midjourney_dataset.csv')\n",
    "\n",
    "df_chat_gpt_prompts = pd.read_csv('E:\\Diploma\\model_training\\datasets\\chatgpt_prompts.csv')\n",
    "\n",
    "df_chat_gpt_4_prompts = pd.read_csv(\"E:\\Diploma\\model_training\\datasets\\chatgpt_dataset.csv\")\n",
    "\n",
    "df_SDXL_prompts = pd.read_csv(\"E:\\Diploma\\model_training\\datasets\\SDXL_prompts.csv\")\n",
    "\n",
    "df_midjourney_prompts_2 = pd.read_csv(\"E:\\Diploma\\model_training\\datasets\\midjourney_prompts_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fbd5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add columns to dataset and choose columns\n",
    "df_sd_formatted = df_sd_prompts[[\"prompt\"]]\n",
    "df_midjourney_prompts_formatted = df_midjourney_prompts.rename(columns={\"extracted\": \"prompt\"})[[\"prompt\"]]\n",
    "df_chat_gpt_prompts_formatted = df_chat_gpt_prompts[[\"prompt\"]]\n",
    "df_chat_gpt_4_prompts_formatted = df_chat_gpt_4_prompts.rename(columns={\"chatml_prompt\": \"prompt\"})[[\"prompt\"]]\n",
    "df_SDXL_prompts_formatted = df_SDXL_prompts.rename(columns={\"prompts\": \"prompt\"})\n",
    "df_midjourney_prompts_2_formatted = df_midjourney_prompts_2.rename(columns={'text': \"prompt\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd3c9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                    id  \\\n",
      "0           0  00000d0e-45cb-47b6-9f72-6a481e940d78   \n",
      "1           1  00001a8f-993f-4d69-8fd2-f7d69dc1e8ef   \n",
      "2           2  00002cfc-8170-4a93-a1f8-aa5681cb5f71   \n",
      "3           3  00004467-fdef-41bc-bc73-20c68444a024   \n",
      "4           4  000044ca-a4d7-46a2-81da-7ef3bf4cbeeb   \n",
      "\n",
      "                                              prompt  \\\n",
      "0  man waking up, dark and still room, cinematic ...   \n",
      "1  Yate con familia feliz navegando por el mar ca...   \n",
      "2  Many friendly alien race individuals. fantasy,...   \n",
      "3  theo james as cyclops, cyberpunk futuristic ne...   \n",
      "4  Portrait of a beautiful woman with long hair o...   \n",
      "\n",
      "                                                 url  width  height  \\\n",
      "0  https://krea-prod-v1-generations.s3.us-east-1....    512     512   \n",
      "1  https://image.lexica.art/full_jpg/00001a8f-993...    640     640   \n",
      "2  https://image.lexica.art/full_jpg/00002cfc-817...    512     768   \n",
      "3  https://image.lexica.art/full_jpg/00004467-fde...    512     768   \n",
      "4  https://image.lexica.art/full_jpg/000044ca-a4d...    512     768   \n",
      "\n",
      "              source_site  \n",
      "0  stablediffusionweb.com  \n",
      "1              lexica.art  \n",
      "2              lexica.art  \n",
      "3              lexica.art  \n",
      "4              lexica.art                                                content  \\\n",
      "0  **<https://s.mj.run/vN6ggUx7POc> --ar 9:16** -...   \n",
      "1  **<https://s.mj.run/QIgF9-KsZaU> cats with man...   \n",
      "2  **<https://s.mj.run/QIgF9-KsZaU> cats with man...   \n",
      "3  **<https://s.mj.run/eejPAL0v0Lc> --ar 9:16** -...   \n",
      "4  **<https://s.mj.run/DaYjUIcQdb0> --ar 9:16** -...   \n",
      "\n",
      "                                           extracted  \n",
      "0                         --ar 9:16** - Upscaled by   \n",
      "1   cats with many eyes floating in colorful glow...  \n",
      "2   cats with many eyes floating in colorful glow...  \n",
      "3                         --ar 9:16** - Upscaled by   \n",
      "4                         --ar 9:16** - Upscaled by       Unnamed: 0                          act  \\\n",

      "0         104                Personal Chef   \n",
      "1         122              PHP Interpreter   \n",
      "2         151                  Proofreader   \n",
      "3          59  AI Trying to Escape the Box   \n",
      "4         135                  Salesperson   \n",
      "\n",
      "                                              prompt  \n",
      "0  I want you to act as my personal chef. I will ...  \n",
      "1  I want you to act like a php interpreter. I wi...  \n",
      "2  I want you act as a proofreader. I will provid...  \n",
      "3  [Caveat Emptor: After issuing this prompt you ...  \n",
      "4  I want you to act as a salesperson. Try to mar...  \n"
     ]
    }
   ],
   "source": [
    "# Choose data in dataframes\n",
    "print(df_sd_prompts.head(), df_midjourney_prompts.head(), df_chat_gpt_prompts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,

   "id": "6d815460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              prompt  Unnamed: 0\n",
      "0  photographic portrait of a stunningly beautifu...         NaN\n",
      "1                                       blue house's         NaN\n",
      "2                        malaga beach with big waves   3417089.0\n",
      "3  Swirls  fog  phantom  ghost  human  glowing re...   1748148.0\n",
      "4  pastel flowers, Sad cat in river, half cat bod...    346191.0\n"
     ]
    }
   ],
   "source": [
    "# Combining datasets\n",
    "combining_datasets = pd.concat([df_sd_formatted, df_midjourney_prompts_formatted,\n",
    "                                df_chat_gpt_prompts_formatted, df_chat_gpt_4_prompts_formatted,\n",
    "                                df_SDXL_prompts_formatted, df_midjourney_prompts_2_formatted], ignore_index=True)\n",
    "combining_datasets = combining_datasets.sample(frac=1, random_state=4214).reset_index(drop=True)\n",
    "print(combining_datasets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277db8d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907953 98161 153 40000 5538811\n"
     ]
    }
   ],
   "source": [
    "# Check dataset\n",
    "print(len(df_sd_formatted), len(df_midjourney_prompts_formatted), len(df_chat_gpt_prompts_formatted), len(df_chat_gpt_4_prompts_formatted), len(combining_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20200ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Diploma\\.venv\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "combining_datasets = pd.read_csv('./datasets/full_dataset.csv')\n",
    "combining_datasets = combining_datasets.drop_duplicates()\n",
    "combining_datasets_chunked = numpy.array_split(combining_datasets, 5)\n",
    "combining_datasets = None\n",
    "for  i, chunk in enumerate(combining_datasets_chunked, 1):\n",
    "    processed_chunk = chunk[chunk['prompt'].str.split().str.len() >= 30]\n",
    "    processed_chunk = processed_chunk[processed_chunk['prompt'].apply(is_english)]\n",
    "    processed_chunk.to_csv(f'processed_chunk_{i}.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5efdc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4760963"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save clean dataset\n",
    "sum([len(length) for length in combining_datasets_chunked])"

   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
