Отлично, я подготовлю раздел проектирования пояснительной записки для платформы интерактивной генерации и оценки запросов к нейросетям, включая технические требования, архитектурные решения, описание пользователей и аппаратные характеристики для развёртывания. Это займёт немного времени, и я сообщу, когда документ будет готов.

## 1. Общие положения

В современную эпоху развития генеративных нейросетевых моделей качество и релевантность получаемых от них результатов во многом зависят от того, насколько хорошо сформулирован пользовательский запрос (так называемый *промпт*). Процесс составления и улучшения таких запросов получил название «промпт-инжиниринг». Для эффективного использования больших языковых моделей (LLM) и моделей генерации изображений пользователям требуется инструмент, позволяющий интерактивно оттачивать формулировки промптов, получать обратную связь о их качестве и видеть примерный результат без длительного цикла проб и ошибок. 

В данной работе рассматривается проектирование платформы, предназначенной для интерактивного формирования, оценки и предварительного просмотра запросов к языковым и генеративным нейросетям. Платформа нацелена на помощь разработчикам и художникам в создании качественных промптов, предоставляя им возможность редактировать запросы (как вручную в виде текста, так и посредством визуального переставления токенов), автоматически дополнять их деталями, получать количественную оценку качества и адаптировать формулировку под конкретную целевую модель, а также осуществлять предварительную генерацию результата для просмотра. Таким образом, пользователь может итеративно совершенствовать свой запрос, улучшая его до тех пор, пока не будет достигнуто желаемое качество, *до* передачи данного промпта основной модели для финальной генерации контента.

Платформа реализуется по классической **клиент-серверной архитектуре** и включает несколько компонентов. **Серверная часть** построена на современном веб-фреймворке FastAPI (Python), обеспечивающем высокопроизводительный REST API для взаимодействия с клиентом и внешними сервисами ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)). В связке с сервером работают встроенные нейросетевые модели: локально развёрнутая языковая модель **LLaMA 8B** для обработки текстовых запросов и внешняя модель генерации изображений, доступная через **API Fusion Brain**. LLaMA (Large Language Model Meta AI) – это большая языковая модель от компании Meta AI, семейство которой включает версии разного размера от 7 до 65 млрд параметров ([LLaMA — Википедия](https://ru.wikipedia.org/wiki/LLaMA#:~:text=LLaMA%20,%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%BC%D1%83%20%D1%81%D0%BE%D0%BE%D0%B1%D1%89%D0%B5%D1%81%D1%82%D0%B2%D1%83%20%D0%B2%D0%B5%D1%81%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9%20LLaMA)). В данном проекте выбрана модель порядка 8 миллиардов параметров (LLaMA 8B) как компромисс между качеством генерируемого текста и вычислительной сложностью. Для генерации примеров изображений интегрируется сервис **Fusion Brain API**, предоставляющий доступ к модели *Kandinsky 3.1*, способной создавать реалистичные изображения на основе текстового описания ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=Fusion%20Brain%20API%20,%D0%BF%D0%BE%20API%2C%20%D1%81%D1%82%D0%B0%D0%BB%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20Kandinsky)). Последняя (разработки Sber AI) – это современная нейросеть для текст-to-image, доступная по API в актуальной версии 3.1 ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=Kandinsky%20,%D0%BE%D1%81%D0%BD%D0%BE%D0%B2%D0%B5%20%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D1%8F%20%D0%BD%D0%B0%20%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%BC%20%D1%8F%D0%B7%D1%8B%D0%BA%D0%B5)). **Клиентская часть** системы реализована как одностраничное веб-приложение (SPA) на основе фреймворка Vue.js, работающего в браузере пользователя. Для хранения данных платформа использует **базу данных PostgreSQL**, где сохраняется информация о пользователях и истории их запросов. 

Целевой аудиторией системы являются **пользователи-разработчики и художники**, которые будут работать с ней на равных правах (единый уровень доступа). Платформа спроектирована с учётом их потребностей: с одной стороны, она предоставляет гибкие инструменты для тонкой ручной настройки запросов, ценимые разработчиками, с другой – наглядный и интуитивно понятный интерфейс, упрощающий работу с текстовым описанием для творческих специалистов. В следующих разделах будут подробно рассмотрены назначение системы, характеристики пользователей, требования, архитектура и используемые решения, обоснованные сформулированными техническими требованиями.

## 2. Назначение системы

Основное назначение разработанной системы – обеспечить пользователям возможность эффективно формировать и улучшать запросы (промпты) для различных нейросетевых моделей генерации текста и изображений. Платформа предназначена для поддержки полного цикла работы с промптом: от начального наброска идеи до получения удовлетворительного чернового результата. Она предоставляет **единое интерактивное пространство** для проектирования промптов, позволяя объединить несколько функций, которые ранее требовали разрозненных инструментов. 

С помощью данной системы пользователи могут создавать и редактировать текстовые описания запросов, итеративно их уточняя и детализируя. Интеллектуальные функции платформы позволяют автоматически дополнять введённый текст недостающими деталями и контекстом, что помогает менее опытным пользователям (например, художникам, не обладающим глубокими навыками работы с языковыми моделями) получить более богатые описания. Встроенный модуль оценки качества анализирует сформулированный запрос и выдает количественный показатель (в диапазоне 0–100), отражающий ориентировочное качество промпта. Это даёт возможность сразу увидеть, насколько хорошо запрос может быть воспринят нейросетью, и при необходимости внести правки до генерации результата. Кроме того, система обеспечивает преобразование формата запроса под требования конкретной целевой модели: например, адаптирует описание под синтаксис, предпочтительный для модели генерации изображений или, наоборот, для диалоговой языковой модели. Наконец, реализован **режим предварительного просмотра (preview)**, позволяющий по нажатию кнопки отправить текущий промпт в тестовом режиме модели (локальной LLaMA или внешней Fusion Brain) и получить пример отклика – сгенерированный фрагмент текста либо эскиз изображения. 

Таким образом, платформа выполняет роль *песочницы для промптов*: разработчики могут отладить запросы для своих AI-модулей, а художники – подобрать оптимальные описания для генерации иллюстраций. За счёт возможности быстро просматривать результат и оценку, существенно сокращается число итераций «вслепую». Пользователи могут корректировать промпт до тех пор, пока не будут удовлетворены оценкой и предпросмотром, и лишь затем использовать его на основной целевой модели (например, в продакшн-системе либо в стороннем сервисе генерации). В итоге применение платформы позволяет повысить **качество конечного генерируемого контента** – будь то текстовые ответы модели или создаваемые изображения – и сделать процесс разработки промптов более **эффективным и предсказуемым**. 

## 3. Характеристика пользователей

Как отмечалось, предполагаемые пользователи системы – это **разработчики** и **художники**, работающие с генеративными нейросетями. При проектировании платформы исходили из того, что эти две категории пользователей обладают разным опытом и целями, но обе нуждаются в удобном инструменте для создания хороших промптов. Уровень доступа в системе для них одинаковый (роль *обычного пользователя*), то есть все функции платформы доступны как разработчикам, так и представителям творческих профессий без каких-либо ограничений. 

**Разработчики** (инженеры, специалисты по машинному обучению, авторы чат-ботов и др.) используют платформу преимущественно для отладки и улучшения текстовых запросов к языковым моделям. Для них важны гибкость и точность: они оценивают промпт с точки зрения логики обработки моделью, предсказывают, приведёт ли данная формулировка к нужному ответу. Разработчики, как правило, технически подкованы, поэтому им удобен режим прямого текстового редактирования промпта, возможность вручную корректировать даже мелкие детали формулировки. Они оценят также числовой показатель качества – метрику, которая поможет количественно сравнить разные варианты запросов. В их рабочем процессе платформа вписывается как инструмент быстрого прототипирования промптов: вместо того чтобы многократно вызывать целевую модель и получать от неё ответы для сравнения, разработчик может в интерактивном режиме довести один запрос до оптимального состояния, глядя на предварительные ответы от тестовой модели (LLaMA 8B) и на рейтинг качества. Получив удовлетворяющий результат, он переносит этот промпт в своё основное приложение или сервис. 

**Художники** и специалисты по визуальному контенту (например, иллюстраторы, дизайнеры, концепт-художники) используют систему в несколько ином ключе – для создания описаний, на основе которых модели типа Stable Diffusion или Kandinsky генерируют изображения. Их цель – добиться, чтобы сформулированный текст точно передавал задуманный визуальный образ и стиль. Многие художники не имеют глубоких технических знаний, поэтому для них критична простота и наглядность интерфейса. Платформа учитывает это, предоставляя интуитивно понятные средства: режим редактирования через *перетаскивание токенов* особенно полезен, так как позволяет им воспринимать промпт не как сплошной текст, а как набор отдельных элементов/слов, из которых складывается описание сцены. Перемещая эти элементы, художник может экспериментировать с композицией фразы, не задумываясь о синтаксисе – система автоматически обновит текст запроса. Функция автоматического дополнения детализацией полезна для художника тем, что нейросеть сама предложит дополнительные детали (например, обстановку, освещение, стилистические прилагательные), которые могут улучшить итоговое изображение. Предпросмотр через API Fusion Brain позволяет им практически мгновенно увидеть черновой результат: например, эскиз сгенерированного изображения по текущему описанию. Это визуальная обратная связь чрезвычайно ценна – художник сразу поймёт, в правильном ли направлении двигается, и при необходимости скорректирует промпт (добавит или уберёт детали, изменит формулировку). 

Важно подчеркнуть, что система **не вводит разграничений** между разработчиками и художниками на уровне функционала. И те, и другие работают в одной среде с идентичными возможностями. Это решение принято исходя из многопрофильности современных команд: нередко разработчики и дизайнеры работают совместно над проектами с AI, и им удобно использовать единый инструмент. Платформа удовлетворяет потребности обоих типов пользователей: обеспечивает достаточно **тонкий контроль** для удовлетворения запросов разработчиков и одновременно остается **дружественной** для творческих специалистов, не требуя от них знаний программирования. Такой подход расширяет потенциальную аудиторию системы и повышает её ценность в междисциплинарных командах. 

## 4. Технические требования

Разработка платформы базируется на предварительно сформулированных технических требованиях, отражающих необходимый функционал и ограничения системы. Требования охватывают как **функциональные возможности**, видимые пользователю, так и **нефункциональные характеристики** (производительность, совместимость, безопасность и др.). Ниже перечислены ключевые требования к системе.

**Функциональные требования** (основной пользовательский функционал платформы):

- **Редактирование запросов в текстовом и графическом режиме.** Система должна предоставлять интерфейс для ввода и правки текста промпта вручную (классическое текстовое поле) и альтернативный интерфейс в виде списка токенов, которые пользователь может перемещать, удалять или заменять. Любое изменение последовательности токенов в графическом режиме должно незамедлительно отражаться в текстовом представлении промпта, и наоборот, обеспечивая двунаправленную синхронизацию. Должна поддерживаться работа с промптом произвольной длины (в разумных пределах, например до 1000 символов) и с различными символами, включая буквы разных алфавитов, цифры, знаки препинания и эмодзи (это важно, так как некоторые модели допускают использование эмодзи в описании изображения ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%97%D0%B0%D0%BF%D1%80%D0%BE%D1%81))). Пользователь должен видеть текст своего запроса и иметь возможность редактировать его удобным для себя способом.

- **Автоматизированное дополнение (расширение) запроса деталями.** Платформа должна по запросу пользователя уметь генерировать расширенную версию введённого промпта, добавляя к нему недостающую детализацию. Фактически, это интеллектуальный помощник: на основе исходного чернового текста запроса система (с помощью внутренней языковой модели) предлагает дополнительные описательные фразы, контекст или уточнения, которые могут сделать запрос более понятным для нейросети. Например, пользователь ввёл краткий запрос «кот сидит на дереве», а система может предложить дополнить: «кот сидит на дереве **в лучах заходящего солнца, вокруг осенний пейзаж**». Метод вызова – нажатие специальной кнопки («Дополнить»), после чего текущий текст промпта вместе с уже введёнными деталями остается, а к нему в конец или по соответствующим местам добавляется сгенерированный моделью текст (пользователь затем может отредактировать результат по своему усмотрению). Требуется, чтобы дополнение работало корректно для разных типов запросов (как описательных, так и вопросительных) и на двух основных языках (русский и английский), учитывая, что целевые модели поддерживают многоязычные запросы.

- **Оценка качества промпта.** В системе реализуется автоматическая оценка составленного пользователем запроса по **шкале от 0 до 100**. Эта оценка носит рекомендательный характер и должна отражать степень соответствия промпта лучшим практикам и ожидаемым требованиям модели. Пользователю отображается числовое значение или графический индикатор (например, цветовая индикация: красный – низкое качество, зелёный – высокое). Внутренне оценка может основываться на наборе правил или модели, анализирующих текст: учитывается длина запроса, специфичность формулировок, наличие деталей, отсутствие противоречий или запретов. Например, слишком короткий или расплывчатый запрос получит низкую оценку, а конкретный, содержательный запрос – более высокую. Чем более **точно и понятно** сформулирован промпт, тем выше должен быть рейтинг. (Это соответствует рекомендациям по промптингу: качество результатов зависит от того, сколько информации предоставлено и насколько хорошо запрос составлен ([Основы промптинга | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/ru/introduction/basics#:~:text=%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D1%82%20%D0%BE%D1%82%20%D1%82%D0%BE%D0%B3%D0%BE%2C%20%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8,%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D1%8C%20%D1%8D%D1%82%D0%B8%20%D1%8D%D0%BB%D0%B5%D0%BC%D0%B5%D0%BD%D1%82%D1%8B%2C%20%D1%87%D1%82%D0%BE%D0%B1%D1%8B%20%D0%BB%D1%83%D1%87%D1%88%D0%B5)).) Данный модуль служит подсказкой: он должен быстро (в течение долей секунды) пересчитывать оценку при изменении текста и тем самым стимулировать пользователя улучшать запрос до получения приемлемого балла. 

- **Реструктуризация запроса под конкретную модель.** Система должна поддерживать преобразование формата промпта в зависимости от выбранной целевой нейросети. Поскольку разные модели могут по-разному интерпретировать один и тот же текст (например, образные фразы или служебные слова), полезно иметь функцию, которая автоматически **адаптирует стиль и синтаксис запроса**. Пользователь указывает, под какую модель нужен запрос (например, переключателем или выпадающим списком: «LLaMA» vs «Fusion Brain/Kandinsky»), после чего система применяет набор правил или алгоритм преобразования. Например, если выбрана модель генерации изображений, то из текста могут быть удалены лишние глаголы повелительного наклонения («нарисуй», «покажи») и добавлены визуальные характеристики или разделены позитивный и негативный промпты. Так, запрос «Сгенерируй картинку: **кот на дереве ночью, не используй яркие цвета**» может быть трансформирован в две части: **основное описание** = «кот на дереве ночью» и **негативный промпт** = «яркие цвета» (специфика API Kandinsky). Если же выбрана языковая модель, наоборот, система может убрать излишне художественные обороты или, скажем, оформить запрос как вопрос для чат-бота. Эта функциональность упрощает пользователю задачу ручного переписывания промптов при переключении между моделями: достаточно один раз создать запрос, а платформа представит его в оптимальном виде для каждой поддерживаемой нейросети.

- **Предварительный просмотр результата (preview).** Одним из важнейших требований является возможность **быстрой генерации чернового результата по текущему запросу**. Пользователь, не покидая интерфейс редактора промптов, должен получить от системы пример отклика целевой модели – сгенерированный **текст или изображение**, соответствующее введённому описанию. Данный функционал реализуется двумя способами: для текстовых моделей – через локальную LLaMA 8B, для изображений – через вызов внешнего API Fusion Brain. Платформа должна определить, какой тип результата требуется (например, по выбранному целевому движку или по контексту: если пользователь редактирует текстовый запрос для чат-бота, то генерируется текст-ответ; если промпт адресован модели изображения, то генерируется картинка). После нажатия кнопки «Preview» происходит обращение к соответствующей модели:  
  - В случае текста: LLaMA 8B генерирует продолжение или ответ на заданный промпт (необходимо ограничить размер выдаваемого фрагмента, например 1000 символов, чтобы получить быстрый предварительный ответ). Полученный ответ отображается на экране (например, в отдельном поле под запросом). 
  - В случае изображения: сервер отправляет запрос к API Fusion Brain (модель *Kandinsky 3.1*) с параметрами генерации (тип = GENERATE, текст запроса и др.). В ответ через некоторое время возвращается сгенерированное изображение, которое выводится пользователю (в виде эскиза, уменьшенного изображения либо полноразмерного, если позволяет интерфейс). Пользователь должен иметь возможность быстро увидеть этот результат *в интерфейсе*, без ручного перехода на внешние сервисы. Например, платформа может отобразить сгенерированную картинку прямо в браузере. 
  В обоих случаях предварительно сгенерированный контент носит ознакомительный характер – он позволяет оценить, что **примерно** получится из данного промпта. Пользователь, проанализировав результат, может тут же подправить запрос и снова вызвать preview, добиваясь улучшения. Важно, чтобы среднее время получения предпросмотра было приемлемым: для текста это обычно менее 5 секунд, для изображения – порядка 5–15 секунд (зависит от мощности сервера и скорости внешнего API). 

Помимо основных функций, вытекают и **общие технические требования** к системе:

- **Интуитивность и удобство интерфейса.** Пользователи разных категорий должны легко освоить работу с платформой. Интерфейс должен быть локализован (минимум на русском языке, так как аудитория – в том числе русскоговорящие художники). Все элементы (кнопки «Оценить», «Предпросмотр», переключатель режима редактирования и т.д.) должны быть наглядно обозначены. Требуется реализация динамического обновления – например, пересчёт оценки качества без перезагрузки страницы, мгновенная синхронизация текстового и токенного представления запроса. Это подразумевает использование возможностей SPA (Vue.js) для реактивности.

- **Производительность и масштабируемость.** Платформа должна эффективно работать при одновременном использовании несколькими пользователями. Ожидается, что число активных пользователей невелико (например, единицы или десятки, поскольку целевая аудитория – команда разработчиков/художников или отдельные специалисты), однако архитектура не должна иметь жёстких ограничений на масштабирование. Серверная часть на FastAPI должна обрабатывать параллельно несколько запросов (благодаря асинхронности и производительности FastAPI это достижимо ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9))). Важно оптимизировать время отклика: интерактивные операции (редактирование, оценка) происходят почти мгновенно, а более тяжёлые (предпросмотр) – максимально быстро для выбранных моделей. Для ускорения генерации текста модель LLaMA загружается в память заранее и остаётся в режиме ожидания запросов, чтобы не тратить время на инициализацию при каждом preview. Для Fusion Brain API можно предусмотреть обработку в асинхронном режиме (не блокируя основной поток приложения во время ожидания ответа от внешнего сервиса). 

- **Ограничения и обработка ошибок.** Система должна корректно обрабатывать нестандартные ситуации: слишком длинные запросы (превышающие лимиты модели) – выдавая предупреждение или автоматически укорачивая до допустимого размера; недопустимые символы – экранируя или удаляя их; отсутствие связи с внешним API – уведомляя пользователя о невозможности получить preview в данный момент. Все входные данные валидируются (FastAPI и Pydantic обеспечивают автоматическую проверку типов и форматов на уровне API ([Как FastAPI обеспечивает высокую производительность ... - Яндекс](https://ya.ru/neurum/c/tehnologii/q/kak_fastapi_obespechivaet_vysokuyu_proizvoditelnost_9dc97334#:~:text=%D0%9A%D0%B0%D0%BA%20FastAPI%20%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B8%D0%B2%D0%B0%D0%B5%D1%82%20%D0%B2%D1%8B%D1%81%D0%BE%D0%BA%D1%83%D1%8E%20%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%B2%D0%BE%D0%B4%D0%B8%D1%82%D0%B5%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D1%8C,Pydantic%20%D0%B4%D0%BB%D1%8F%20%D0%BF%D1%80%D0%BE%D0%B2%D0%B5%D1%80%D0%BA%D0%B8%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85))), а при обнаружении некорректных – сервер возвращает понятные сообщения об ошибках. Важным требованием является **устойчивость**: сбой одной из компонентов (например, недоступность БД или падение модели) не должен приводить к неуправляемому краху всего приложения – должны быть предусмотрены механизмы возврата в консистентное состояние или перезапуска сервиса.

- **Совместимость и развиваемость.** Платформа должна быть построена с использованием стандартных веб-технологий, обеспечивающих её совместимость с основными браузерами (Chrome, Firefox, Safari, Edge) без необходимости установки специальных плагинов. Желательно соблюдение принципов адаптивности интерфейса (возможность работать на экране ноутбука, настольного ПК, и, по возможности, на планшете). В архитектуре и коде должны быть заложены возможности расширения: например, добавление новых моделей (если в будущем понадобится поддержать другую нейросеть для генерации аудио или видео, или добавить ещё одну языковую модель) не должно требовать переписывания системы с нуля. Это означает, что компоненты (модуль оценки, модуль реструктуризации) должны быть реализованы конфигурируемо, с возможностью подключения новых правил или алгоритмов.

Сформулированные выше требования служат основой для проектирования системы. На их основании в следующих разделах выбираются соответствующие методики и технические решения, обосновывается архитектура, алгоритмы работы и необходимые ресурсы. Соблюдение этих требований должно обеспечить, что итоговая платформа будет полнофункциональной, удобной для целевых пользователей и надёжной в эксплуатации.

## 5. Архитектура системы

**Общая архитектура.** Платформа имеет распределённую архитектуру типа «клиент–сервер». Это означает, что функциональность разделена между фронтендом (клиентским приложением, выполняющимся в браузере пользователя) и бекендом (серверным приложением, выполняющим бизнес-логику, взаимодействие с моделями и БД). На **рисунке 1** представлена структурная схема архитектуры системы с основными компонентами и потоками данных между ними.

 ([image]()) *Рис. 1. Архитектура системы, отображающая взаимодействие основных компонентов платформы.* 

Как показано на схеме, центральным звеном является **веб-сервер API**, реализованный на FastAPI. Этот сервер принимает входящие запросы от клиентского приложения по протоколу HTTP(S) и отвечает на них. Взаимодействие клиента и сервера происходит по REST API: для каждой функции предусмотрен соответствующий **эндпойнт**. Например, могут быть маршруты `/complete_prompt` (для дополнения детализации), `/evaluate_prompt` (для оценки), `/transform_prompt` (для реструктуризации) и `/preview` (для получения результата генерации). Клиентское приложение обращается к этим конечным точкам, передавая необходимые данные (тексты промптов, параметры) в формате JSON, и получает от сервера ответы, также закодированные в JSON (кроме случаев, когда передаются двоичные данные изображения). Обмен происходит по сети, предпочтительно с использованием шифрования (HTTPS) для безопасности.

**Клиентская часть (Vue.js SPA)** – это одностраничное приложение, загружаемое в браузер. Оно отвечает за отображение UI и интерактивность. После загрузки (HTML, CSS, JS) клиент устанавливает связь с сервером через AJAX-запросы (например, с помощью `fetch` API или библиотеки axios) к REST API. Вся логика по обновлению интерфейса – на стороне клиента: перемещение токенов, обновление текста, визуализация оценки (например, индикатор качества) – выполняется средствами JavaScript внутри браузера. Vue.js, являясь реактивным фреймворком, упрощает реализацию динамических компонентов интерфейса ([Vue.js — Википедия](https://ru.wikipedia.org/wiki/Vue.js#:~:text=Vue,24%20%D0%B2%20%D1%80%D0%B5%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%BC%20%D1%81%D1%82%D0%B8%D0%BB%D0%B5)). Например, текстовое поле и список токенов могут быть связанными реактивными данными: изменение одного автоматически отражается в другом. Клиент также обрабатывает элементы управления – кнопка «Оценить» может просто вызывать соответствующую функцию JS, которая отправит запрос на сервер и по получении ответа обновит отображаемую оценку. Аналогично, нажатие «Preview» инициирует последовательность: показать индикатор загрузки, отправить промпт на сервер, дождаться ответа и затем либо отобразить сгенерированный текст, либо встроить полученное изображение (например, через создание HTML-элемента `<img>` с полученными данными). Таким образом, фронтенд выполняет роль **презентационного слоя**, обеспечивая удобство для пользователя и минимизируя задержки взаимодействия (многие операции, не требующие ресурсов сервера, происходят мгновенно за счёт реактивности Vue).

**Серверная часть (FastAPI)** – **сердце бизнес-логики** платформы. FastAPI выбран благодаря его современной архитектуре, оптимизированной под высоконагруженные API и удобство разработки ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)). Приложение FastAPI запускается, как правило, под управлением ASGI-сервера (например, Uvicorn), способного обрабатывать асинхронные запросы. Основные компоненты на стороне сервера:
- **Контроллеры (роуты) API.** В FastAPI разработаны обработчики для каждого метода API. Они принимают входные данные (автоматически распарсенные из JSON благодаря Pydantic-моделям), выполняют требуемые действия (например, вызывают модель или делают запрос к БД) и формируют ответ. Каждый такой контроллер работает по принципу: получить запрос -> вызвать соответствующую внутреннюю функцию/метод -> вернуть результат пользователю. Например, контроллер `/evaluate_prompt` получив JSON с полем `prompt: "текст запроса"`, передаст эту строку в модуль оценки качества и вернёт клиенту JSON с полем `score: 78` (пример). 
- **Модуль интеграции с LLaMA.** Поскольку модель LLaMA 8B развёрнута локально, серверное приложение должно загружать и хранить эту модель в памяти, а также иметь методы для генерации текста. При запуске сервера происходит инициализация: с помощью библиотеки PyTorch и соответствующих средств (например, Hugging Face Transformers) модель загружается из файла весов. Благодаря интеграции напрямую в приложение, это называется *встроенным (embedded) подходом* – модель включена в состав сервиса и доступна как объект в коде ([Building a Machine Learning Microservice with FastAPI | NVIDIA Technical Blog](https://developer.nvidia.com/blog/building-a-machine-learning-microservice-with-fastapi/#:~:text=Image%3A%20Simple%20diagram%20of%20an,representation%20of%20the%20embedded%20approach)). Например, может быть создан singleton-объект `llama_model`, который потом используется различными ручками API. Модель, находящаяся в оперативной памяти на GPU, может обрабатываться быстрыми вызовами – это значительно ускоряет получение ответа (не требуется каждый раз поднимать отдельный процесс или обращаться к удалённому API). Взаимодействие с LLaMA происходит через вызовы функций генерации: сервер формирует на основе пользовательского промпта вход для модели и вызывает метод `model.generate()` (либо аналогичный, в зависимости от используемой библиотеки), получая результат – сгенерированный текст. Этот текст затем отфильтровывается/обрабатывается по необходимости (например, обрезается до нужной длины) и пересылается клиенту.
- **Модуль интеграции с Fusion Brain API.** В случаях, когда требуется сгенерировать изображение, сервер выполняет роль клиента к внешнему API. Он делает HTTP-запрос (например, POST) к службе Fusion Brain, включая в него текст промпта и необходимые параметры (версия модели, желаемое разрешение изображения и т.д.). В ответ Fusion Brain возвращает данные изображения. Согласно документации, Kandinsky 3.1 позволяет получать изображения размером до 1024×1024 ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%A0%D0%B0%D0%B7%D0%BC%D0%B5%D1%80%D1%8B%20%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9)); для предпросмотра сервер может запрашивать, к примеру, изображение 512×512 пикселей, чтобы сократить время ответа. Полученное изображение может приходить закодированным (например, URL ссылки на изображение или бинарные данные). Сервер, получив результат, преобразует его в формат, пригодный для пересылки фронтенду. Наиболее прямолинейный способ – переслать изображение как набор байт (в base64) или как ссылку, проксируемую через сервер. В реализации платформы может использоваться проксирование: сервер сохраняет картинку во временном хранилище (или просто держит в памяти) и отдаёт фронту по тому же API (например, ответ на `/preview` для изображения содержит URL вида `/media/preview123.png` или непосредственный Base64). Клиентская часть затем отображает картинку пользователю. **Взаимодействие с внешним API должно быть безопасным**: ключ API Fusion Brain хранится на сервере (в конфигурации), и ни при каком условии не отправляется на клиент. Запросы должны выполняться с учётом возможных задержек (лучше асинхронно, чтобы не блокировать другие процессы сервера).

- **Модуль работы с базой данных.** Серверное приложение взаимодействует с PostgreSQL для хранения постоянных данных. Это включает регистрацию/авторизацию пользователей (см. раздел 9 о безопасности) и сохранение истории запросов. История запросов может храниться в виде таблицы: пользователь, текст промпта, время, возможно оценка и предпросмотр (например, ссылка на сохранённый результат). Обращения к базе данных реализованы через ORM (например, SQLAlchemy) либо через драйвер `asyncpg` для асинхронной работы. Использование ORM обеспечивает защиту от SQL-инъекций и удобство разработки – запросы к базе генерируются автоматически, что повышает надёжность хранения данных ([PostgreSQL — Википедия](https://ru.wikipedia.org/wiki/PostgreSQL#:~:text=PostgreSQL%20%28%D0%BF%D1%80%D0%BE%D0%B8%D0%B7%D0%BD%D0%BE%D1%81%D0%B8%D1%82%D1%81%D1%8F%20%C2%AB%D0%9F%D0%BE%D1%81%D1%82,%D0%A1%D0%A3%D0%91%D0%94)) ([PostgreSQL: что это за СУБД, основы и преимущества](https://blog.skillfactory.ru/glossary/postgresql/#:~:text=PostgreSQL%20%E2%80%94%20%D1%8D%D1%82%D0%BE%20%D0%BE%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D0%BD%D0%BE,%D1%8F%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%D1%81%D1%8F%20%D0%B0%D0%BB%D1%8C%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D1%82%D0%B8%D0%B2%D0%BE%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%BC%D0%B5%D1%80%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%BC%20%D0%B1%D0%B0%D0%B7%D0%B0%D0%BC%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)). PostgreSQL, будучи одной из наиболее развитых свободных СУБД ([PostgreSQL: что это за СУБД, основы и преимущества](https://blog.skillfactory.ru/glossary/postgresql/#:~:text=PostgreSQL%20%E2%80%94%20%D1%8D%D1%82%D0%BE%20%D0%BE%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D0%BD%D0%BE,%D1%8F%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%D1%81%D1%8F%20%D0%B0%D0%BB%D1%8C%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D1%82%D0%B8%D0%B2%D0%BE%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%BC%D0%B5%D1%80%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%BC%20%D0%B1%D0%B0%D0%B7%D0%B0%D0%BC%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)), гарантирует целостность данных и поддерживает все необходимые функции (ACID-транзакции, индексацию и пр.), что важно для ведения истории запросов без потерь.

**Взаимодействие компонентов.** Все компоненты связаны через чётко определённые интерфейсы. Браузерный клиент обращается к серверу только через API (не напрямую к БД или моделям). Сервер, в свою очередь, взаимодействует с LLaMA через вызовы функций (в памяти процесса) и с Fusion Brain через HTTP, а с базой – через SQL-запросы. Подобная модульность облегчает замену частей: например, можно обновить версию модели LLaMA или переключиться на другую языковую модель, не затрагивая логику фронтенда; либо заменить Fusion Brain на иной API генерации изображений, скорректировав лишь соответствующий модуль. 

Архитектура поддерживает масштабирование: при возросшей нагрузке фронтенд-приложение может быть вынесено на отдельный CDN, а серверная часть – развёрнута в виде нескольких экземпляров за балансировщиком нагрузки. База данных может быть отдельно вынесена на кластер PostgreSQL для обеспечения высокой доступности. Так как сервер Stateless (не хранит сессии в памяти, вся важная информация – в БД), то масштабировать его горизонтально несложно. Единственный ограничивающий фактор – модель LLaMA, которая потребляет значительный объем GPU-ресурсов, поэтому в случае интенсивной нагрузки возможно использование нескольких серверов с каждой загруженной моделью (или альтернатива – вынесение модели в отдельный сервис). Но в расчёте на целевое применение (ограниченное число пользователей) текущая архитектура (один бекенд-инстанс с моделью) считается достаточной и оптимальной по простоте.

## 6. Алгоритмическое и программное обеспечение

В данном разделе описаны ключевые алгоритмы, реализующие функции платформы, а также используемое программное обеспечение (библиотеки, фреймворки) и принятые при разработке технические решения. Проектирование алгоритмов велось на основе требований, сформулированных в разделе 4, – каждый функциональный пункт соответствует определённой методике реализации. Также рассматриваются программные средства, обеспечивающие выполнение этих алгоритмов.

**Редактирование запросов (текст и токены).** Алгоритм редактирования в текстовом режиме тривиален: пользователь вводит или изменяет строку символов, которая хранится во внутреннем состоянии приложения (например, в переменной `promptText` в Vue.js). Более интересен алгоритм представления промпта в виде **набора токенов**. Здесь под токеном понимается либо отдельное слово, либо устойчивое словосочетание – на практике для упрощения можно принять токенизацию по пробелам и знакам пунктуации (каждое слово или знак рассматривается как отдельный элемент). При переключении в режим токенов исходный текст промпта разбивается на массив токенов. Этот массив отображается на экране, например, в виде последовательности интерактивных «карточек» с текстом. Для реализации перетаскивания (drag-and-drop) используются возможности HTML5 DnD или готовые компоненты из экосистемы Vue (например, Vue.Draggable). Алгоритм перетаскивания следующий: когда пользователь начинает перенос токена, система запоминает исходный индекс; при бросании токена в новую позицию происходит изменение порядка элементов массива. Далее массив конкатенируется обратно в строку с добавлением пробелов – таким образом получается обновлённый текст промпта. Vue.js автоматически отследит изменение массива токенов и обновит связанную переменную текста (или наоборот, если связность сделана через одно хранилище состояния). Таким образом обеспечивается одновременная актуализация обоих представлений. Кроме того, если пользователь в текстовом поле ручным образом изменяет текст (например, дописывает новое слово), алгоритм распознаёт это (через реактивный watcher) и пересобирает список токенов заново. В результате редактирование может происходить параллельно в любом формате без рассинхронизации. Этот алгоритм достаточно прост (основные операции – разбиение строки и объединение списка), поэтому не нагружает систему даже при длинных промптах.

**Дополнение запроса детализацией.** Данная функция опирается на алгоритмы генерации текста с помощью модели LLaMA 8B. При нажатии пользователем кнопки «Дополнить» фронтенд отправляет текущий текст промпта на сервер (эндпойнт `/complete_prompt`). Сервер вызывает специальную функцию, которая обращается к загруженной модели LLaMA. Алгоритм можно описать так:
1. **Формирование входной подсказки (prompt) для модели.** Чтобы модель поняла задачу, используется шаблон: например, модель может быть проинструктирована следующим образом – *«Расширь описание, добавив детали: [исходный промпт]»*. Если LLaMA является инструкционно-дообученной (instruct-tuned) моделью, ей можно напрямую дать команду дописать описание. Если же нет, можно подготовить несколько примеров (few-shot) или использовать простое продолжение: например, сгенерировать текст, используя исходный промпт как начало, ожидая, что модель естественным образом продолжит описание дополнительными подробностями.
2. **Генерация текста моделью.** Путём вызова `llama_model.generate(input_ids, max_new_tokens=N, ...)` модель генерирует продолжение текста. Здесь `max_new_tokens` – ограничение на длину дополнения (например, N=50 токенов, чтобы получить 1–2 предложения дополнения). Используются параметры декодирования: температурой можно слегка увеличить разнообразие, чтобы дополнение не было слишком однообразным.
3. **Обработка результата.** Модель возвращает сгенерированный фрагмент текста. Его нужно интегрировать в исходный промпт. Можно просто добавить его в конец строки (после исходного текста). Например, было: «кот сидит на дереве», модель вернула « ночью при свете луны, вокруг лес темнеет». Объединяем: «кот сидит на дереве ночью при свете луны, вокруг лес темнеет». В некоторых случаях может потребоваться небольшая правка стыковки (например, убрать дублирующее слово, привести в согласованное время и падежи – эту задачу в прототипе можно упростить, доверив модели генерировать уже грамматически продолженный текст).
4. **Возврат дополненного текста клиенту.** Клиент получает новую строку и обновляет текст промпта на экране.

Алгоритмически, основная интеллектуальная работа здесь выполняется языковой моделью. Предусмотрены некоторые меры для улучшения результата: например, если модель начала повторять исходный текст или уводит в сторону, сервер может отсечь первые токены генерации до появления новых данных. Также при реализации данного алгоритма используется специальное программное обеспечение – библиотека Hugging Face Transformers (или аналог) для работы с моделью. Поскольку LLaMA 8B не предоставляется готовым REST API, она интегрирована напрямую. PyTorch обеспечивает необходимый функционал генерации на GPU. Таким образом, **алгоритм дополнения промпта** сводится к задаче последовательного **генерирования текста** нейросетью – типичная задача, для которой используются наработки и функции библиотек машинного обучения.

**Оценка качества промпта.** Алгоритм оценки – это комбинация эвристических правил и, потенциально, моделей машинного обучения. В рамках данного проекта реализуется более простой, объяснимый подход на основе **метрик текста**. При вызове функции оценки (пользователь нажимает, либо автоматически при изменениях) фронтенд отправляет промпт на сервер (`/evaluate_prompt`). Сервер вызывает функцию `evaluate_prompt(text)`, которая выполняет следующие шаги:
1. **Анализ длины и полноты.** Вычисляется длина текста (в словах или символах). Если длина меньше некоторого порога (например, менее 3–5 слов), выставляется низкий базовый балл (такие запросы, как правило, слишком общие). Оптимальный диапазон длины – эмпирически, скажем, 10–20 слов для текстовых моделей, 5–15 слов для графических (со стилевыми тегами). Если запрос очень длинный (более 50–100 слов), тоже может снижаться балл – за избыточность.
2. **Проверка специфичности.** Алгоритм может содержать список «пустых» слов (типа *«изобрази», «сделай»* – паразитные для промпта генерации изображения) и, наоборот, проверить наличие содержательных прилагательных, уточняющих фраз. Например, наличие хотя бы одного прилагательного или определяющего оборота может добавлять балл, так как уточняет запрос. В руководствах по промптингу подчёркивается, что специфичность и контекст повышают качество результата ([Основы промптинга | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/ru/introduction/basics#:~:text=%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D1%82%20%D0%BE%D1%82%20%D1%82%D0%BE%D0%B3%D0%BE%2C%20%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8,%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D1%8C%20%D1%8D%D1%82%D0%B8%20%D1%8D%D0%BB%D0%B5%D0%BC%D0%B5%D0%BD%D1%82%D1%8B%2C%20%D1%87%D1%82%D0%BE%D0%B1%D1%8B%20%D0%BB%D1%83%D1%87%D1%88%D0%B5)). Следовательно, если промпт содержит детали (например, указание стиля, времени суток, эмоций персонажа и т.п.), это положительно влияет на оценку.
3. **Ясность формулировки.** Проверяется, нет ли в тексте двусмысленностей или нерелевантных фрагментов. Этот пункт сложнее формализовать, но можно частично оценить по структуре предложения: если промпт состоит из нескольких несвязанных предложений или вопросительных форм (которые могут сбить модель), балл снижается. Например, запрос вида: «Нарисуй кота. Может дерево? Нет, лучше солнце.» явно плох по структуре – алгоритм может обнаружить наличие нескольких предложений или вопросительных знаков и снизить оценку.
4. **Учет модели.** Если уже выбрана целевая модель, алгоритм может использовать разные профили оценки. Например, для изображения ценятся указания на визуальный стиль (реалистичный, мультяшный, 3D-рендер и т.д.) – их наличие повышает балл. Для текста ценится чётко поставленный вопрос или задача – наличие вопросительного знака при обращении к чат-боту, наоборот, может быть положительным (для диалоговой модели).
5. **Формирование итогового балла.** Балл вычисляется на основании нескольких компонент. Например: `score = length_score + specificity_score + clarity_score + model_adaptation_score`. Каждая в диапазоне 0–25, суммируя до 100. Данный подход позволяет объяснить оценку: *«Ваш промпт слишком короткий, добавьте деталей»* – если низкий length_score, *«промпт содержит противоречивые указания»* – если clarity_score низкий, и т.п. (В перспективе, можно подключить и саму LLaMA для оценки – например, попросив её оценить промпт по шкале, но это менее детерминированно.)
6. **Возврат балла.** Сервер возвращает число от 0 до 100. Клиент отображает его, при этом возможно сопроводить коротким вердиктом (например, «Среднее качество» при 50–70, «Отличный промпт» при >80). 

Этот алгоритм легко модифицировать под новые критерии, его можно обучить (если бы были данные промптов с оценками, можно настроить веса правил или модель классификатор). Пока же он реализован на основе здравого смысла и рекомендаций по написанию промптов ([Искусство написания промптов для ChatGPT и других LLM моделей](https://vc.ru/chatgpt/1228166-iskusstvo-napisaniya-promptov-dlya-chatgpt-i-drugih-llm-modelei-26-principov-napisaniya-promptov-dlya-polucheniya-otlichnogo-rezultata#:~:text=%D0%98%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%BE%20%D0%BD%D0%B0%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D1%8F%20%D0%BF%D1%80%D0%BE%D0%BC%D0%BF%D1%82%D0%BE%D0%B2%20%D0%B4%D0%BB%D1%8F%20ChatGPT,%D0%92)) ([Основы промптинга | Prompt Engineering Guide<!-- --> ](https://www.promptingguide.ai/ru/introduction/basics#:~:text=%D0%B7%D0%B0%D0%B2%D0%B8%D1%81%D0%B8%D1%82%20%D0%BE%D1%82%20%D1%82%D0%BE%D0%B3%D0%BE%2C%20%D1%81%D0%BA%D0%BE%D0%BB%D1%8C%D0%BA%D0%BE%20%D0%B8%D0%BD%D1%84%D0%BE%D1%80%D0%BC%D0%B0%D1%86%D0%B8%D0%B8,%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D1%8C%20%D1%8D%D1%82%D0%B8%20%D1%8D%D0%BB%D0%B5%D0%BC%D0%B5%D0%BD%D1%82%D1%8B%2C%20%D1%87%D1%82%D0%BE%D0%B1%D1%8B%20%D0%BB%D1%83%D1%87%D1%88%D0%B5)). Программно реализация выполнена на Python: для лингвистического анализа можно использовать стандартные библиотеки (например, NLTK или simple pos-tagging для выделения частей речи, если нужно искать прилагательные). Однако в простейшем варианте достаточно операций над строками, что не требует внешних зависимостей. Важно отметить, что **время выполнения** этого алгоритма невелико – порядка нескольких миллисекунд, т.к. текст анализируется без тяжёлых моделей. Поэтому оценка может обновляться практически мгновенно при каждом изменении запроса.

**Реструктуризация промпта под модель.** Алгоритм реструктуризации заключается в применении определённого **набора правил трансформации текста** в зависимости от целевой модели. Эти правила основаны на известных требованиях/особенностях промптов для разных генераторов. Реализация может быть как шаблонной (набор if/else в коде), так и с участием нейросети (например, снова задействовать LLaMA: «Переформулируй запрос для такой-то модели»). В первом прототипе достаточно правила:
- Для **модели изображения (Fusion Brain/Kandinsky)**: 
  - Удалить обращение к модели в повелительном наклонении. Например, фразы вроде «нарисуй», «сгенерируй картинку:» не несут содержания для модели, их следует убрать.
  - Выделить негативные указания. Если в тексте есть конструкции «не используй ...», «без ...», они преобразуются в отдельный негативный промпт (для Kandinsky предусмотрено поле negativePromptUnclip ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=cdn))). Алгоритм: найти ключевые слова «не», «без», собрать последующие слова до запятой или конца – это будет отрицательный промпт.
  - Проверить язык описания. Kandinsky 3.1 понимает и русский, и английский ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%92%D1%8B%20%D0%B8%20%D0%B2%D0%B0%D1%88%D0%B8%20%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D0%B8%20%D0%BC%D0%BE%D0%B3%D1%83%D1%82,%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20emoji%20%D0%B2%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2%D0%BE%D0%BC%20%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D0%B8)). Здесь не нужно перевода, но важно убедиться, что промпт цельный на одном языке (смешение языков нежелательно).
  - Возможно, добавить стандартные атрибуты, если их нет: например, многие художники добавляют стилистические теги («арт», «реалистично», «аниме-стиль»). Если пользователь ничего про стиль не указал, алгоритм мог бы по умолчанию добавить что-то нейтральное (но чаще стиль лучше оставить пустым – Fusion Brain и так принимает параметр style, который по умолчанию = NONE).
  - Итогом алгоритма станет либо изменённая строка промпта (если мы решим не разделять, а просто убрать лишние слова), либо структура с полями: main_prompt и negative_prompt. Сервер знает, что для FusionBrain надо использовать оба. На стороне клиента это может отобразиться или остаться скрытым – по решению.
- Для **языковой модели (LLaMA или подобные)**:
  - Если исходный промпт больше походил на описание картинки (например, перечисление объектов без вопроса), а цель – текст, возможно, стоит преобразовать его в форму задания или вопроса. Например, промпт «кот сидит на дереве ночью» для чат-бота LLM непонятен – алгоритм мог бы добавить «Опиши сцену: кот сидит на дереве ночью.» либо обернуть это в вопрос: «Что произошло: кот сидит на дереве ночью? Расскажи историю.» Но такие преобразования могут оказаться слишком творческими. Вероятно, лучше минимально менять текст.
  - Удалить специфические теги или слова, характерные для графических запросов (типа «4K», «highly detailed») – в текстовой модели они бессмысленны.
  - Если целевая языковая модель – диалоговая, можно добавить, например, пометку *«User: {prompt}\nAssistant:»* для внутренних нужд (в LLaMA 2 есть формат входа с метками ролей).
  
В целом алгоритм реструктуризации во многом состоит из *поиска и замены*. Он оперирует строкой: ищет определённые шаблоны (регулярные выражения или ключевые фразы) и преобразует их. Его сложность невысока, реализация на Python с библиотекой `re` или просто методами строк. При расширении на новые модели (например, аудиогенерация) нужно будет добавить соответствующие правила (например, убрать слова «изображение», заменить их на «звук», и т.д.).

**Предварительный просмотр результата.** Алгоритмы preview тесно связаны с интеграцией моделей, но стоит рассмотреть их с точки зрения последовательности шагов:
- **Preview текста (LLaMA)**: Пользователь нажимает «Предпросмотр» когда выбран тип = «текст» (например, работает над промптом для чат-бота). Браузер вызывает `/preview` с параметром `type=text`. Сервер получает текст запроса. Если необходимо, он может дополнительно контекстуализировать промпт (например, добавить системное сообщение для LLaMA, чтобы та отвечала кратко). Затем вызывается метод генерации LLaMA, очень похожий на используемый для дополнения: `model.generate` с определёнными параметрами. Отличие – максимальная длина ответа обычно больше (например, до 100 токенов), так как нужно видеть некоторый осмысленный фрагмент. Чтобы получить **предсказуемый** отклик, можно снизить температуру до 0.7 или использовать **детерминированный decoding** (top_p=0.9, либо вообще greedy) – тогда каждый preview будет повторяемым. По окончании генерации сервер возвращает сгенерированный текст. Клиент отображает его, например, ниже поля ввода, в специальном блоке «Предварительный ответ модели». Алгоритмически это простая цепочка: вызов нейросети -> форматирование ответа -> отдача. Если генерация занимает заметное время (несколько секунд), можно реализовать асинхронный механизм: сразу ответить клиенту кодом 202 и периодически опрашивать состояние, но в рамках небольшого приложения достаточно синхронного ожидания (браузер и так покажет спиннер).
- **Preview изображения (Fusion Brain)**: При preview графики браузер аналогично делает запрос `/preview` с `type=image`. Сервер осуществляет последовательность:
  1. Применяет алгоритм реструктуризации для графической модели к тексту промпта (чтобы учесть особенности – см. выше). Получает откорректированный текст и, возможно, отдельно негативный промпт.
  2. Формирует запрос к API Fusion Brain. Согласно документации, для генерации изображения нужно отправить JSON с полями типа, размер, сам запрос и др. Например: `{"type": "GENERATE", "query": "<описание>", "num_images": 1, "style": "<стиль>"}` и т.д. (Сервис находится в бета-версии, но уже позволяет через API получать результаты ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=Fusion%20Brain%20API%20,%D0%BF%D0%BE%20API%2C%20%D1%81%D1%82%D0%B0%D0%BB%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20Kandinsky)) ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%92%20%D0%BD%D0%B0%D1%81%D1%82%D0%BE%D1%8F%D1%89%D0%B5%D0%B5%20%D0%B2%D1%80%D0%B5%D0%BC%D1%8F%20Kandinsky%20%D0%B4%D0%BE%D1%81%D1%82%D1%83%D0%BF%D0%B5%D0%BD,%D1%80%D0%B5%D0%B6%D0%B8%D0%BC%D0%B5%20%D0%BF%D0%BE%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B5%20%D1%81%20%D0%B8%D0%B7%D0%BE%D0%B1%D1%80%D0%B0%D0%B6%D0%B5%D0%BD%D0%B8%D1%8F%D0%BC%D0%B8)).) 
  3. Отправляет HTTP-запрос. Используется модуль `requests` или асинхронный `httpx`. К запросу обязательно прикрепляется **API-ключ** (например, в заголовке `Authorization`), полученный разработчиком от Fusion Brain. 
  4. Ждёт ответа. API может отвечать либо URL ссылки на готовое изображение (которое нужно затем скачать), либо сразу бинарными данными изображения. Предположим, он возвращает URL (например, ссылка на CDN). Тогда сервер делает второй запрос – скачивает изображение по этой ссылке. 
  5. После получения изображения сервер может изменить его размер или формат при необходимости (например, конвертировать в JPEG, если нужно). 
  6. Затем сервер отправляет изображение клиенту. Как отправить: вариант 1 – вернуть URI, по которому клиент потом сам заберёт картинку; вариант 2 – закодировать изображение в base64 и вернуть как строку (что увеличивает размер JSON, но избавляет от дополнительных запросов). С учётом того, что изображение preview не слишком большое, base64-строка размером ~0.5–1 МБ допустима. Однако более элегантно – реализовать endpoint для статических изображений: например, сохранить файл `preview_<session_id>.png` на сервере (в каталоге static) и вернуть клиенту JSON с путём `/static/preview_<...>.png`. Клиент затем установит этот путь в src тега `<img>` для отображения.
  
  В прототипе можно упростить и действительно вернуть base64: фронтенд, получив эту строку, создаёт элемент `<img src="data:image/png;base64,<строка>" />`, мгновенно показывая картинку без дополнительного трафика.
  
  7. Пользователь видит сгенерированное изображение прямо в интерфейсе. Он может продолжить редактирование и заметить, как картинка меняется при разных промптах – это реализует интерактивный подбор оптимального описания.
  
Алгоритмически часть, завязанная на Fusion Brain, — это **внешний вызов**. Возможные проблемы: сетевые задержки, отказ в обслуживании (например, превышение квоты API). Алгоритм предусматривает тайм-аут (например, 10 секунд на генерацию). Если по истечении этого времени результат не получен, сервер отменяет запрос и возвращает клиенту ошибку или сообщение о тайм-ауте. Клиент информирует пользователя (например, «Не удалось получить ответ от сервиса генерации. Попробуйте позже.»). 

**Программное обеспечение и инструменты разработки.** Для реализации описанных алгоритмов применяются следующие основные технологии:
- *Backend:* язык **Python 3.10+**, фреймворк **FastAPI** – обеспечивает быстрый старт в создании API, поддержку асинхронности и автоматическую генерацию API-документации (OpenAPI/Swagger). Использование FastAPI способствует созданию устойчивых и понятных программных решений за счёт применения аннотаций типов и Pydantic-моделей ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)). Для запуска сервера выбран **Uvicorn** – легковесный ASGI-сервер. 
- *ML-библиотеки:* **PyTorch** (CUDA) для загрузки и использования LLaMA 8B. PyTorch – де-факто стандарт для работы с нейросетями в Python, что облегчает интеграцию; модель LLaMA доступна в виде весов, которые загружаются через HuggingFace Transformers или официальную реализацию. Возможна также оптимизация с помощью библиотеки **transformers** (HuggingFace), предоставляющей высокоуровневый API для генерации (Model.generate) и токенизации (Tokenizer.encode/decode). 
- *HTTP-клиенты:* для вызова внешнего API используется **requests** (синхронный HTTP-клиент) либо **httpx** (асинхронный). В целях упрощения можно применить requests, завернув вызов в `await loop.run_in_executor`, чтобы не блокировать основной поток. 
- *Database:* **PostgreSQL** – в связке с Python часто применяют **SQLAlchemy** как ORM. В проекте можно использовать **Tortoise ORM** или даже простой `asyncpg` драйвер. ORM упрощает описание моделей данных (например, класс User, PromptHistory) и транзакционные операции. 
- *Frontend:* **Vue.js 3** – прогрессивный фреймворк JS. Код клиентской части пишется на HTML + CSS + JavaScript (ES6). Возможно использование **TypeScript** для лучшей поддержки типов. Vue.js предоставляет реактивное двухстороннее связывание данных, что удобно для нашего редактирования промпта. Например, можно создать компонент `<PromptEditor>` с свойством `promptText` в `data()`, и вычисляемым свойством `tokens` (геттер и сеттер, которые разбивают и объединяют строку). При перетаскивании элементов списка, `tokens` сеттер обновит `promptText`, и Vue автомагически обновит текстовое поле. Также Vue облегчает разделение интерфейса на компоненты: отдельный компонент для поля ввода текста, отдельный – для списка токенов, для оценки, для отображения изображения и т.д. Используется механизм **Vue CLI** или Vite для сборки проекта – в результате получаются статические файлы (`index.html`, `app.js`, `style.css`), которые разворачиваются на сервере.
- *Дополнительные библиотеки фронтенда:* возможно применение UI-библиотеки компонентов (например, Vuetify или Element Plus) для стилизованных элементов (кнопок, индикаторов, прогресс-баров). Однако можно и вручную оформить CSS-стили под нужды приложения, тем более интерфейс относительно специфичный (drag-n-drop).
- *Управление зависимостями:* используется **poetry** или **pip** + `requirements.txt` для фиксирования версий библиотек. В требованиях к серверу будут числиться fastapi, uvicorn, torch, transformers, requests, sqlalchemy, psycopg2 и т.д.
- *Среда разработки:* для написания кода применялись стандартные IDE (например, PyCharm для бэкенда, Visual Studio Code с плагинами для фронтенда). Код структурирован: в бэкенде – модули `api` (с контроллерами), `services` (с логикой: evaluator.py, completer.py, etc.), `models` (ORM-модели БД), `core` (инициализация приложения, загрузка модели). В фронтенде – разбиение на компоненты Vue (.vue файлы), хранилище (Vuex/Pinia, если нужно хранить глобальное состояние, например, текущий пользователь) и сервисы для вызова API (можно создать модуль api.js с методами `evaluate(prompt)` и пр., использующими axios).

Следует отметить, что выбранные технологии ориентированы на **быструю разработку и поддержку**. Так, Vue.js и FastAPI минимизируют количество шаблонного кода – это позволило сосредоточиться на реализации алгоритмов (редактирования промпта, оценки, взаимодействия с моделями). Интеграция с ML-моделями на PyTorch также достаточно прямая, учитывая обилие примеров и документации. По сути, благодаря этим инструментам удалось перейти от идеи к рабочему прототипу весьма оперативно ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=%D0%98%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8F%20FastAPI%20%D1%81%C2%A0%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%BC%D0%B8%20%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0%D0%BC%D0%B8%20%D0%B4%D0%BB%D1%8F,%D0%B2%D1%8B%D1%81%D0%BE%D0%BA%D0%BE%D0%B3%D0%BE%20%D1%83%D1%80%D0%BE%D0%B2%D0%BD%D1%8F%20%D0%BC%D0%B0%D1%81%D1%88%D1%82%D0%B0%D0%B1%D0%B8%D1%80%D1%83%D0%B5%D0%BC%D0%BE%D1%81%D1%82%D0%B8%20%D1%80%D0%B0%D0%B7%D1%80%D0%B0%D0%B1%D0%B0%D1%82%D1%8B%D0%B2%D0%B0%D0%B5%D0%BC%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)), при этом архитектура остается масштабируемой и профессионально пригодной. 

Подводя итог, алгоритмическое обеспечение платформы сочетает в себе:
- Классические алгоритмы обработки строк и структур данных (для редактирования и реструктуризации промптов).
- Применение мощи предобученных моделей (LLaMA, Kandinsky) для генеративных задач – эти алгоритмы основаны на методах глубокого обучения и реализуются с помощью соответствующего ПО.
- Эвристические методы оценки качества, опирающиеся на экспертизу в области prompt engineering.
- Современные практики веб-разработки для обеспечения всего этого функционала через удобный пользовательский интерфейс.

Все эти компоненты гармонично объединены в рамках выбранной архитектуры и технологического стека, что подтверждает реализуемость поставленных требований и готовность системы к дальнейшей реализации и испытаниям.

## 7. Конструктивно-технологические решения

Разработка программного обеспечения всегда сопряжена с выбором не только алгоритмов, но и конкретных технологий, а также способов их интеграции и развертывания. В данном разделе рассматриваются решения, касающиеся архитектуры системы в техническом (конструктивном) плане и организационно-технологические аспекты: структура развёртывания компонентов, способ обеспечения их совместной работы, обоснование выбранных технологий с точки зрения инфраструктуры и будущей поддержки.

**Логическая и физическая структура системы.** Платформа разделена на несколько компонентов, как описано в архитектуре: фронтенд (Vue.js), сервер (FastAPI + модели) и база данных (PostgreSQL). **Логически** они взаимодействуют через чёткие интерфейсы (HTTP API, SQL). **Физически** для упрощения развертывания на практике они могут быть размещены на одном сервере или виртуальной машине, либо на нескольких, в зависимости от требований к производительности:
- *Вариант 1:* Все компоненты на одном мощном сервере (с GPU). Этот вариант подходит для небольшой команды пользователей. Сервер с GPU запускает FastAPI-приложение (которое, в свою очередь, загружает модель LLaMA на GPU), на этом же сервере работает служба PostgreSQL и раздаются статика фронтенда. Преимущество – минимальные накладные расходы на коммуникацию (всё локально) и простота настройки.
- *Вариант 2:* Разделение на два сервера: один – **вычислительный** (GPU) для FastAPI + LLaMA, второй – **общий** для фронтенда и БД. Этот вариант может быть оправдан, если GPU-хост работает под Linux без оконной системы и предназначен исключительно для ML-задач, а веб-сервер/UI и база данных удобнее разместить на другом (например, облачном) сервисе. Связь между ними пойдёт по сети (FastAPI-сервер на GPU будет доступен по API для фронтенда). Однако тогда Fusion Brain API будет вызываться с GPU-сервера (внешний интернет должен быть доступен).
- *Вариант 3:* Полное разделение: фронтенд хостится на CDN или в облаке (GitHub Pages, Netlify и т.п.), FastAPI + модель – на GPU-сервере, PostgreSQL – на отдельном DB-сервере (например, управляемая БД в облаке). Это более сложная, но масштабируемая архитектура, применимая, если число пользователей растёт и нужно разнести нагрузку.

Для целей дипломного проекта предполагается **локальное развертывание** (вариант 1): на одной машине запускаются все нужные сервисы. Это упрощает демонстрацию и тестирование. 

**Использование контейнерных технологий.** Для облегчения установки и переноса системы был выбран подход с применением **Docker-контейнеров**. Разработаны Dockerfile для каждого сервиса:
- Образ для **FastAPI сервера** на базе `python:3.10-slim`. В образ включается установка необходимых Python-библиотек (из requirements.txt). Также, чтобы использовать GPU внутри контейнера, применяется NVIDIA Docker runtime (на целевой машине должен быть установлен NVIDIA Docker). Контейнер при запуске монтирует volume с моделями (чтобы не пересобирать образ при добавлении весов LLaMA) или включает скачивание модели.
- Образ для **Vue.js фронтенда** на базе `node:18` (для сборки) и nginx (для отдачи). Процесс: на этапе build контейнера выполняется `npm install && npm run build`, полученные статические файлы копируются на минималистский образ с nginx, который настроен раздавать содержимое `/usr/share/nginx/html` (туда помещён `index.html` и ассеты). Nginx также может быть настроен проксировать запросы `/api/` к бекенду FastAPI (если фронтенд и бекенд раздельно хостятся).
- Образ для **PostgreSQL** можно взять готовый (официальный `postgres:14-alpine`), указав переменные окружения для пароля и инициализации БД.

Контейнеризация обеспечивает единообразие среды (исключает проблему «у меня работает, у вас нет»), а также облегчает деплоймент на сервер. Компоненты запускаются оркестратором (например, **Docker Compose** прописывает три сервиса: web, api, db, и нужные сети между ними).

**Выбор технологий и обоснование.** Принятые технологические решения обусловлены требованиями к функционалу и ограничениями ресурсов:
- **FastAPI vs альтернативы:** рассматривались Flask и Django. Flask слишком низкоуровневый и требует больше ручной работы по структуре проекта и валидации данных. Django избыточен (целый MVC-фреймворк) для задачи создания API. FastAPI же специально создан для API, поддерживает асинхронность и даёт высокую производительность на уровне, близком к Node.js или Go благодаря использованию Uvicorn/Starlette ([Чем хорош FastAPI - Easyoffer](https://easyoffer.ru/question/579#:~:text=%D0%A7%D0%B5%D0%BC%20%D1%85%D0%BE%D1%80%D0%BE%D1%88%20FastAPI%20,7%2B%20%D0%B4%D0%BB%D1%8F%20%D0%BE%D0%B1%D0%B5%D1%81%D0%BF%D0%B5%D1%87%D0%B5%D0%BD%D0%B8%D1%8F)). Кроме того, его встроенные механизмы (взаимодействие с Pydantic) упрощают безопасную обработку входных данных (что хорошо для безопасности – см. раздел 9) и автоматическое документирование. Достоинства FastAPI – **высокая производительность и простота** ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)) – стали решающими факторами.
- **PyTorch (LLaMA) vs внешние API:** Одним из ключевых решений было использовать локальную модель LLaMA 8B вместо обращения к, например, OpenAI API для текстовых генераций. Во-первых, это обеспечивает **автономность** – система может работать полностью офлайн (что актуально, если данные пользователей конфиденциальны и их нельзя отправлять во внешние сервисы). Во-вторых, локальная модель позволяет настроить работу так, как нужно (например, ограничить длину ответа, получить скрытые состояния, если требуется). И хотя качество LLaMA 8B может быть ниже GPT-3.5, для целей предпросмотра и оценки промптов она справляется (тем более, что она из семейства моделей Meta, известных хорошим качеством при оптимальном количестве параметров ([LLaMA — Википедия](https://ru.wikipedia.org/wiki/LLaMA#:~:text=LLaMA%20,%D0%B8%D1%81%D1%81%D0%BB%D0%B5%D0%B4%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D1%8C%D1%81%D0%BA%D0%BE%D0%BC%D1%83%20%D1%81%D0%BE%D0%BE%D0%B1%D1%89%D0%B5%D1%81%D1%82%D0%B2%D1%83%20%D0%B2%D0%B5%D1%81%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D0%B5%D0%B9%20LLaMA))). Это решение потребовало наличия GPU, но в разделе 8 обосновываются необходимые аппаратные ресурсы – они достижимы. Кроме того, **открытость кода** LLaMA позволяет при необходимости тонко донастроить модель на специфичные данные (в будущем, например, можно обучить её лучше оценивать промпты).
- **Fusion Brain API vs локальная генерация изображений:** Для генерации изображений был выбран готовый API (Fusion Brain, предоставляющий доступ к модели Kandinsky 3.1 ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=Fusion%20Brain%20API%20,%D0%BF%D0%BE%20API%2C%20%D1%81%D1%82%D0%B0%D0%BB%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20Kandinsky))). Альтернативой могло быть поднятие локального сервиса Stable Diffusion или самой модели Kandinsky. Однако это потребовало бы дополнительных **больших вычислительных ресурсов** (еще один крупный вес модели, отдельный GPU). С учётом ограничений оборудования и времени, было рациональнее воспользоваться внешним сервисом. *Fusion Brain* выбран, так как это российская платформа, свободно доступная для использования (модель Kandinsky бесплатна и поддерживает русский язык ([Kandinsky: создать изображение](https://sitelabs.ru/blog/kandinsky/#:~:text=%D0%9E%D0%BD%D0%B0%20%D0%B1%D0%B5%D1%81%D0%BF%D0%BB%D0%B0%D1%82%D0%BD%D0%B0%20%D0%B8%20%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%B8%D0%B2%D0%B0%D0%B5%D1%82%20%D1%80%D1%83%D1%81%D1%81%D0%BA%D0%B8%D0%B9,%D0%B8%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BC%D0%B5%D0%BD%D1%82%20%D0%B4%D0%BB%D1%8F%20%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8F%20%D1%86%D0%B8%D1%84%D1%80%D0%BE%D0%B2%D0%BE%D0%B3%D0%BE%20%D0%B8%D1%81%D0%BA%D1%83%D1%81%D1%81%D1%82%D0%B2%D0%B0))), что соответствует целевой аудитории. Также, Kandinsky 3.1 – одна из самых продвинутых моделей генерации изображений на момент разработки, демонстрирует высокое качество (почти на уровне MidJourney в ряде случаев). Использование её API позволяет пользователям получать **высококачественные превью изображений без необходимости самим иметь мощный GPU**. Таким образом, это решение – компромисс между качеством и сложностью: перенести нагрузку по генерации изображений на удалённый сервис. Конечно, от сервиса требуется интернет-соединение, но в современных условиях это приемлемо.
- **Vue.js vs React/Angular:** Для фронтенда выбрана Vue.js из-за её более низкого порога вхождения и удобства для постепенного внедрения. Разработчики, особенно знакомые с HTML/CSS, обычно легче осваивают Vue, тогда как React требует писать больше шаблонного кода, а Angular – слишком громоздок для небольшого проекта. Vue известен своей доступностью и отличной документацией ([Vue.js — Википедия](https://ru.wikipedia.org/wiki/Vue.js#:~:text=Vue,24%20%D0%B2%20%D1%80%D0%B5%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%BC%20%D1%81%D1%82%D0%B8%D0%BB%D0%B5)). Кроме того, Vue прекрасно подходит для реализации drag-and-drop интерфейсов, существующие плагины и сообщество упрощают эту задачу. В итоге, **Vue.js обеспечивает быструю разработку интерактивного интерфейса** при небольших затратах времени.
- **PostgreSQL vs SQLite/Другие СУБД:** Для хранения данных выбрана PostgreSQL, хотя объем сохраняемой информации невелик. Обоснование – промышленная надежность и масштабируемость. PostgreSQL – открытая ORDBMS с более чем 30-летней историей разработки, одна из самых продвинутых БД с открытым кодом ([PostgreSQL: что это за СУБД, основы и преимущества](https://blog.skillfactory.ru/glossary/postgresql/#:~:text=PostgreSQL%20%E2%80%94%20%D1%8D%D1%82%D0%BE%20%D0%BE%D0%B1%D1%8A%D0%B5%D0%BA%D1%82%D0%BD%D0%BE,%D1%8F%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%D1%81%D1%8F%20%D0%B0%D0%BB%D1%8C%D1%82%D0%B5%D1%80%D0%BD%D0%B0%D1%82%D0%B8%D0%B2%D0%BE%D0%B9%20%D0%BA%D0%BE%D0%BC%D0%BC%D0%B5%D1%80%D1%87%D0%B5%D1%81%D0%BA%D0%B8%D0%BC%20%D0%B1%D0%B0%D0%B7%D0%B0%D0%BC%20%D0%B4%D0%B0%D0%BD%D0%BD%D1%8B%D1%85)), поддерживающая строгие гарантии сохранности данных. В случае расширения проекта (например, ведение большой базы промптов, статистики) PostgreSQL справится без проблем. Кроме того, использовать ее удобно в Docker-контейнере, а также многие ORM поддерживают ее «из коробки». SQLite могла бы быть проще на этапе прототипа, но для веб-приложения с потенциально несколькими пользователями параллельно SQLite не так надёжен (он однопоточный). Поэтому предпочтение отдано PostgreSQL – **надежность данных и возможность роста**.

**Организация разработки и деплоймента.** При реализации данного проекта был использован подход непрерывной интеграции: код хранится в системе контроля версий (Git), настроено автоматическое построение Docker-образов при обновлении. Это облегчает тестирование на разных машинах (можно быстро развернуть весь стек с помощью `docker-compose up`). Также применялись практики разбиения задач: UI и backend разрабатывались отдельно, через согласованный API (для проверки использовались инструменты типа Swagger UI, сгенерированный FastAPI автоматически, что помогало фронтенд-разработчику видеть, какие запросы доступны). 

**Безотказность и отказоустойчивость.** Конструктивно, система спроектирована с учетом потенциальных отказов:
- Если падает внешний сервис (Fusion Brain), платформа всё ещё функционирует частично (редактирование, оценка, даже текстовое preview через LLaMA продолжает работать). Пользователю выдаётся сообщение об ошибке внешнего сервиса, но само приложение не «рушится».
- Если по какой-то причине недоступна модель LLaMA (например, не хватило памяти и процесс упал), FastAPI возвращает код ошибки при попытке её использовать, а интерфейс может предупредить, что текстовый preview временно недоступен. При этом другие функции (хранение истории, редактирование) работают.
- База данных – единая точка хранения. В случае сбоя БД (например, отключилась) сервер не сможет записать/прочитать историю или выполнить логин. Такие ситуации обрабатываются: сервер выдаст ошибку 500, а на UI можно отобразить уведомление «Сервис временно недоступен». Чтобы повысить надежность, в продуктивной эксплуатации СУБД обычно запускают с механизмами репликации, регулярного бэкапа. Для дипломного прототипа достаточно периодически сохранять дамп БД и, при необходимости, перезапустить контейнер БД (данные не потеряются благодаря использованию volume для хранения файлов БД на хосте).

**Масштабируемость и будущие улучшения.** Заложенные технические решения обеспечивают возможность развития платформы:
- Добавление новых типов генеративных моделей (например, подключение API для генерации музыки или 3D) потребует добавить новые модули, но общая архитектура (плагиноподобная) это допускает. Например, можно расширить энум `target_model` и написать новые правила реструктуризации и preview.
- Интерфейс может быть расширен: например, добавить отдельные поля для выбора стиля изображения, которые будут отправляться Fusion Brain API (этот API поддерживает параметр `style` ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%A1%D1%82%D0%B8%D0%BB%D1%8C))). Конструктивно, UI на Vue легко обновляется компонентами, а API – модифицируется, поскольку использованы гибкие инструменты (FastAPI позволяет быстро добавить новое поле в Pydantic-модель запроса).
- Производительность можно нарастить: если нужно больше параллелизма, можно запустить несколько копий FastAPI-приложения за nginx-балансировщиком. Модель LLaMA 8B в каждом из процессов займет память, но можно использовать 2 GPU (каждый процесс привязан к своему GPU) – тогда одновременно обслуживается больше запросов. Это конструктивное решение: сейчас приложение однопроцессное, но Uvicorn можно запустить с, например, 2 worker-процессами. Да, модель придется загрузить дважды (под каждий), но если есть ресурсы – это даёт масштабирование по вертикали.
- Безопасность тоже масштабируется: при необходимости ввести разграничение прав (скажем, администратор может смотреть все промпты пользователей), архитектура БД уже позволяет хранить роли, а FastAPI – проверять credentials и вводить condition в endpoints. То есть изначальный упор на единый уровень доступа выбран для простоты, но не препятствует добавлению RBAC в будущем.

**Обеспечение качества и тестирование.** Технологические решения способствуют тестируемости: модульная структура (отдельно функции для оценки, для дополнения) позволяет писать unit-тесты на них. Можно было использовать фреймворк pytest для бэкенда, и мокировать ответы модели (чтобы тесты не зависели от фактической генерации). Frontend-тестирование возможно средствами Jest или Cypress для e2e, хотя в рамках диплома акцент сделан на функциональность, поэтому основное тестирование проводилось вручную: проверка сценариев пользователя (см. раздел 9 на предмет безопасности) и нагрузочное тестирование с несколькими одновременными запросами.

В итоге, принятые конструктивно-технологические решения – использование контейнеризации, разделение на компоненты, выбор оптимального стека – обеспечили **надежную и гибкую основу** для реализации платформы. Этот подход облегчает как текущую разработку (каждый компонент можно развернуть и отладить независимо), так и дальнейшее сопровождение (обновление отдельного сервиса не нарушит работу остальных при соблюдении контракта API). Все решения согласованы с требованиями: например, необходимость GPU для LLaMA связана с требованием локальной генерации текста, а привлечение Fusion Brain – с требованием высококачественного preview изображений. Такое сочетание решений подтверждает **целесообразность и реализуемость** проекта в заданных условиях.

## 8. Аппаратные требования

Платформа для интерактивной работы с нейросетями предъявляет определённые требования к аппаратному обеспечению, обусловленные, главным образом, ресурсными потребностями языковой модели LLaMA 8B и необходимостью стабильной работы серверной части. Ниже приводятся минимальные и рекомендуемые характеристики оборудования для развёртывания системы, раздельно по компонентам.

**Сервер для запуска LLaMA 8B (PyTorch).** Модель LLaMA с ~8 миллиардами параметров является крупной и для эффективного функционирования нуждается в графическом ускорителе (GPU):
- **GPU:** Требуется современная дискретная видеокарта NVIDIA с объёмом памяти не менее *16 ГБ*. Согласно практическим оценкам, модель LLaMA 7B в полном 32-битном представлении занимает ~28 ГБ VRAM, а при использовании 16-битного сжатия (FP16) – около 14 ГБ, что как раз помещается в 16-гигабайтный буфер GPU ([LLaMA 7B GPU Memory Requirement - Transformers - Hugging Face Forums](https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323#:~:text=To%20run%20the%207B%20model,the%20model%20on%20a%20T4)). Модель 8B близка по размерам к 7B, ожидаемо требуя ~16 ГБ в FP16. Таким образом, **минимум** – видеокарта уровня NVIDIA Tesla T4 (16 GB), RTX A5000 (24 GB) или GeForce RTX 3090/4080 (24/16 GB). Оптимально – 24 ГБ VRAM, чтобы модель могла загружаться с запасом и, возможно, позволяла параллельно обрабатывать несколько запросов. GPU должна поддерживать архитектуру CUDA (для PyTorch) и иметь вычислительную способность, достаточную для запуска Transformer-модели (любые CUDA-совместимые, начиная с серии 10х0, но по памяти ограничение выше).
- **CPU:** Хотя основная нагрузка на GPU, центральный процессор обрабатывает вспомогательные задачи – запуск сервера, подготовку данных, часть вычислений PyTorch (например, слоёвая нормализация может частично выполняться на CPU, если GPU загружен). Рекомендуется минимум 4 ядра (8 потоков) на частоте ~2.5 ГГц и выше. Например, Intel Core i7 или AMD Ryzen 5. Это обеспечит плавную работу Uvicorn-сервера и быстрый запуск процессов. При одновременной работе нескольких пользователей и отсутствии второго GPU, CPU также может брать на себя генерацию текстов (PyTorch позволяет CPU inference, но значительно медленнее). Поэтому **для резерва** желательно иметь CPU с 8+ ядрами и поддержкой AVX2, чтобы модель на CPU работала приемлемо (впрочем, 8B на CPU все равно будет весьма медленной, порядка 1-2 токена в секунду).
- **RAM:** Оперативная память системы нужна для размещения: копии модели (помимо VRAM, модель занимает место в системной RAM при загрузке), кода приложения, БД и прочих служб. Рекомендуемый объем – **32 ГБ ОЗУ**. Минимум – 16 ГБ, но в этом случае может возникать своппинг при одновременной работе модели и СУБД. Например, сам PyTorch при загрузке модели может занять ~8–10 ГБ RAM (буфер с весами, особенно если идут конвертации), плюс память на кэширование запросов Fusion Brain, плюс БД (несколько сотен МБ под процессы). 32 ГБ обеспечат комфортный запас.
- **Носитель данных:** Желательно использовать **SSD** (твердотельный накопитель) ёмкостью от 50 ГБ. Сама модель LLaMA 8B в формате FP16 занимает порядка 8–12 ГБ на диске (в зависимости от оптимизаций), плюс место на ОС, Docker-образах, БД. SSD ускорит загрузку модели при старте (на порядок быстрее HDD), что уменьшит время запуска сервера. Кроме того, при генерации изображений Fusion Brain может кэшировать что-то на диск (например, временные файлы). 50 ГБ – минимально, лучше 100 ГБ, чтобы было место для логов и возможных обновлений.
- **Сеть:** Для взаимодействия с Fusion Brain API сервер должен иметь доступ в интернет. Скорость соединения влияет на скорость получения изображений – желательно широкополосный канал (от 10 Мбит/c и выше). Также, если клиенты подключаются удалённо к серверу, необходимо устойчивое подключение с низкой задержкой. Локальная сеть гигабитная – оптимально, если фронтенд и бэкенд на разных машинах.

**Серверная часть (FastAPI) без учёта модели.** Если предположить сценарий, когда модель LLaMA работает на отдельной машине, а веб-сервер и вспомогательные сервисы – на другой (например, вынесены в облако), то требования к серверу FastAPI значительно скромнее:
- **CPU:** Достаточно 2 ядер (например, виртуальный сервер с 2 vCPU), частота не критична (>=2.0 ГГц). Этого хватит, чтобы обрабатывать веб-запросы, сериализовать/десериализовать JSON и делать сетевые вызовы.
- **RAM:** Основное потребление памяти – самим приложением (несколько сотен мегабайт) и кэшем HTTP запросов. 2–4 ГБ ОЗУ достаточно. Если на этом же узле крутится PostgreSQL, лучше 4 ГБ.
- **Disk:** Под логи и файлы БД – 20 ГБ SSD. БД истории запросов по объему невелика (каждый запрос – текст ~1 КБ максимум, даже 10000 запросов – это порядка 10 МБ текста). Добавим место под саму PostgreSQL, ОС – 20 ГБ с головой.
- **Сеть:** Очень важна стабильность соединения между этим сервером и GPU-сервером (если они разные). Лучше размещать их в одном центре обработки, чтобы задержки были минимальны. Для внешних клиентов – стандартный HTTPS-порт 443, желательно выделенный IP.

**Фронтенд (Vue.js) и клиентские требования.** 
- **Серверная часть фронтенда:** Если UI-файлы раздаются тем же FastAPI (например, через статические роуты) или nginx, сам сервер должен иметь возможность отдавать статические файлы. Это не требует много ресурсов, разве что места на диске (несколько мегабайт). В нашем случае front скомпилирован – ~ few MB JS/CSS.
- **Клиентская сторона (браузер пользователя):** Поскольку система ориентирована на разработчиков и художников, предполагается использование настольных компьютеров или ноутбуков. Требования:
  - Современный браузер: Google Chrome, Mozilla Firefox, Microsoft Edge или Safari актуальной версии. Приложение использует ES6 и Vue 3, что поддерживается этими браузерами. Internet Explorer не поддерживается.
  - Устройство: ПК или ноутбук с как минимум 4 ГБ ОЗУ (браузеру нужно ~200–300 МБ на приложение, остальное – ОС; 4 ГБ хватает для комфортной работы). Разрешение экрана 1366×768 и выше (интерфейс с картинкой превью удобнее на широком экране, минимально HD).
  - Процессор клиента не критичен (большую часть тяжёлой работы делает сервер), но для плавности анимаций drag-and-drop и отрисовки vue-компонентов достаточно любого процессора с ~2 ГГц (даже 2-х ядерного Pentium). Большинство современных ПК удовлетворяют этому.
  - Графическая карта на клиенте не имеет значения, веб-приложение не делает 3D-графики или WebGL, обычного интегрированного видео хватает.
  - Сетевое подключение пользователя: нужно для общения с сервером. Объемы данных: текстовые запросы (пренебрежимо мало), изображения предпросмотра (каждая ~0.5 МБ). Если пользователь активно генерирует много превью, трафик может быть несколько мегабайт в минуту. Любое интернет-подключение (даже мобильный 3G) формально достаточно, но для комфорта лучше высокоскоростное (например, 10 Мбит/с) чтобы изображения загружались за доли секунды. Задержка в 100–200 мс до сервера не критична (время генерации всё равно больше).

**База данных PostgreSQL.** 
- Если БД развёрнута на отдельном сервере (например, Managed Database), минимальная конфигурация – 1 vCPU, 1 ГБ RAM. PostgreSQL сама по себе может работать и на меньших ресурсах для малых объёмов, но 1 ГБ обеспечивает достаточный файловый кэш. 
- Диск: несколько гигабайт, 10 ГБ достаточно с большим запасом. Обязательно SSD (для быстрого доступа и транзакций, хотя с нашей маленькой нагрузкой и HDD справится, но облачные БД обычно SSD по умолчанию).
- Сеть: Желательно нахождение в одной сети с приложением (чтобы задержки обращения к БД минимальны, <1 мс). Если БД в облаке, то обеспечивается внутренний VLAN. Если приложение и БД на одном хосте, то через сокет – вообще без сетевой задержки.

**Резюме аппаратных требований:**

- *Рекомендуемая конфигурация единого сервера:*  
  - GPU: NVIDIA 24 ГБ (например, RTX 3090/4090 или A5000/A6000).  
  - CPU: 8-core, 3.0 GHz.  
  - RAM: 32 GB.  
  - SSD: 100 GB.  
  - Сетевое подключение: 100 Мбит/с Ethernet, интернет с низкой задержкой.

- *Минимальная конфигурация:*  
  - GPU: NVIDIA с 16 ГБ (Tesla T4, RTX 3080 16GB, etc.) – позволит запустить модель в сжатом виде.  
  - CPU: 4-core, 2.5 GHz.  
  - RAM: 16 GB (можно 16, но при работе модель+БД может начать использовать swap).  
  - SSD/HDD: 50 GB (SSD предпочтительнее).  
  - Сеть: 10 Мбит/с.

- *Для разделённого развертывания:*  
  - GPU-сервер: как минимум NVIDIA 16GB GPU, 4-core CPU, 16 GB RAM, 50 GB SSD.  
  - Web-сервер: 2-core CPU, 4 GB RAM, 20 GB SSD (без GPU).  
  - DB-сервер: 1-core CPU, 1–2 GB RAM, 10 GB SSD.

Отдельно отметим, что при необходимости запуска платформы в **отсутствие GPU**, можно воспользоваться оптимизацией модели LLaMA 8B – например, произвести 8-битную или 4-битную квантизацию весов. Это сократит требуемую память и позволит запустить генерацию на CPU или на меньшей видеокарте. Однако скорость резко упадёт (генерация одного ответа может занять десятки секунд). Поэтому для полноценного функционирования **наличие GPU с достаточной памятью является критически важным требованием**. Это следует из природы задачи – мы работаем с большими нейросетями. 

Вместе с тем, использование внешнего API для изображений сняло необходимость второго мощного GPU для диффузионной модели – тут достаточно доступного интернета. Таким образом, совокупные аппаратные требования остаются в разумных пределах для лаборатории или компании, работающей с AI: один производительный рабочий сервер или облачная VM с GPU. 

С точки зрения пользователя-клиента, аппаратные требования минимальны – любой современный компьютер. Это демократизирует доступ: художник может пользоваться платформой со своего ноутбука, не имея у себя локально никаких мощных ресурсов, поскольку все тяжёлые вычисления происходят на сервере.

## 9. Безопасность и аутентификация

При проектировании платформы уделяется серьёзное внимание вопросам безопасности – как защите данных пользователей, так и устойчивости системы к злонамеренным воздействиям. Также реализованы механизмы аутентификации, гарантирующие доступ к функционалу только авторизованным лицам (в рамках данного проекта – зарегистрированным пользователям с ролью разработчика/художника). Ниже описаны меры, принятые для обеспечения безопасности и процессы аутентификации пользователей.

**Аутентификация пользователей.** Доступ к системе предполагается по учетным записям. Каждый пользователь должен пройти процедуру входа (логин) с использованием предварительно зарегистрированных учетных данных (например, email или имя пользователя + пароль). Реализовано следующее:
- На стороне сервера создана таблица `users` в базе данных, содержащая информацию о пользователях: уникальный идентификатор, логин (имя пользователя) и **хеш пароля**. Прямое хранение паролей в базе не допускается – вместо этого применяется крипустойкий алгоритм хеширования. Выбор пал на функцию **bcrypt**, которая специально разработана для безопасного хранения паролей (адаптивная, использует соль и настраиваемую сложность) ([bcrypt — Википедия](https://ru.wikipedia.org/wiki/Bcrypt#:~:text=bcrypt%C2%A0%E2%80%94%20%D0%B0%D0%B4%D0%B0%D0%BF%D1%82%D0%B8%D0%B2%D0%BD%D0%B0%D1%8F%20%D0%BA%D1%80%D0%B8%D0%BF%D1%82%D0%BE%D0%B3%D1%80%D0%B0%D1%84%D0%B8%D1%87%D0%B5%D1%81%D0%BA%D0%B0%D1%8F%20%D1%85%D0%B5%D1%88,%D0%B7%D0%B0%D0%BC%D0%B5%D0%B4%D0%BB%D0%B8%D1%82%D1%8C%2C%20%D1%87%D1%82%D0%BE%D0%B1%D1%8B%20%D1%83%D1%81%D0%BB%D0%BE%D0%B6%D0%BD%D0%B8%D1%82%D1%8C%20%D0%B0%D1%82%D0%B0%D0%BA%D1%83%20%D0%BF%D0%B5%D1%80%D0%B5%D0%B1%D0%BE%D1%80%D0%BE%D0%BC)). При регистрации нового пользователя сервер генерирует соль и вычисляет bcrypt-хеш пароля, который и сохраняется. Таким образом, даже в случае компрометации базы данных исходные пароли не будут раскрыты в явном виде. (FastAPI обладает интеграцией с библиотекой PassLib для работы с bcrypt ([Встроенные механизмы безопасности фреймворков Python / Хабр](https://habr.com/ru/companies/ussc/articles/851578/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D0%BB%D0%B0%D0%B3%D0%B0%D0%B5%D1%82%20%D0%B8%D0%BD%D1%82%D0%B5%D0%B3%D1%80%D0%B0%D1%86%D0%B8%D1%8E%20%D1%81%20%D0%B1%D0%B8%D0%B1%D0%BB%D0%B8%D0%BE%D1%82%D0%B5%D0%BA%D0%B0%D0%BC%D0%B8%2C,%D0%B2%D1%81%D1%82%D1%80%D0%BE%D0%B5%D0%BD%D0%BD%D1%8B%D1%85%20%D0%B8%D0%BD%D1%81%D1%82%D1%80%D1%83%D0%BC%D0%B5%D0%BD%D1%82%D0%BE%D0%B2%20%D0%B4%D0%BB%D1%8F%20%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D1%8B%20%D1%81)), что упрощает реализацию данной функциональности.)
- При входе пользователя (endpoint `/login`) сервер запрашивает логин и пароль, затем находит соответствующую запись пользователя и проверяет корректность пароля, вычислив хеш от введенного пароля и сравнив с хранящимся (bcrypt позволяет быстро проверить пароль, так как соль и параметр сложности сохранены в самом хеше). Если данные не совпадают – доступ отклоняется.
- После успешной аутентификации пользователю выдается токен сессии. Выбрана схема **JWT (JSON Web Token)** для безсессионной аутентификации. JWT содержит зашифрованные данные о пользователе (например, его id и роль) и подписан секретным ключом сервера. Клиентское приложение получает JWT-токен и сохраняет его (в памяти или в localStorage браузера). Все дальнейшие запросы от клиента к API отправляются с этим токеном в HTTP-заголовке `Authorization: Bearer <token>`. Сервер, получая запрос, проверяет подпись JWT и извлекает информацию о пользователе – так понимает, кто делает запрос. Такая схема широко применяется, она сочетает **простоту реализации и достаточную безопасность** для веб-приложений ([Вы кто такие, я вас не знаю, или Как мы делаем JWT-аутентификацию / Хабр](https://habr.com/ru/companies/doubletapp/articles/764424/#:~:text=%D0%9F%D0%BE%D1%87%D1%82%D0%B8%20%D0%B2%D0%BE%20%D0%B2%D1%81%D0%B5%D1%85%20%D0%BD%D0%B0%D1%88%D0%B8%D1%85%20%D0%BF%D1%80%D0%BE%D0%B5%D0%BA%D1%82%D0%B0%D1%85,%D1%80%D0%B5%D0%B0%D0%BB%D0%B8%D0%B7%D0%B0%D1%86%D0%B8%D0%B8%20%D0%B8%20%D0%B1%D0%B5%D0%B7%D0%BE%D0%BF%D0%B0%D1%81%D0%BD%D0%BE%D1%81%D1%82%D1%8C%20%D0%B4%D0%BB%D1%8F%20%D0%BF%D1%80%D0%B8%D0%BB%D0%BE%D0%B6%D0%B5%D0%BD%D0%B8%D0%B9)). В FastAPI есть готовые зависимости для проверки JWT-токенов, что облегчает внедрение.
- JWT-токен имеет ограниченное время жизни (например, 24 часа), после чего требует повторного входа (рефреш). Это предотвращает несанкционированный доступ в случае, если токен был похищен или пользователь забыл выйти на чужом компьютере.
- В рамках дипломного прототипа, возможно, регистрация пользователей не делается публичной (учетные записи создаются администратором заранее), поэтому защита от массовых регистраций или ботов не актуальна. Если бы система была публичной, можно было бы добавить **CAPTCHA** при регистрации/входе, ограничение числа попыток входа (чтобы предотвратить brute-force перебор пароля), подтверждение email и т.д.

**Разграничение доступа.** На данный момент все аутентифицированные пользователи имеют одинаковые права (могут редактировать свои промпты, получать preview и т.д.). Однако система спроектирована с возможностью введения ролей. Например, можно добавить роль *администратор*, которому доступны дополнительные сведения (просмотр истории всех пользователей, управление учётными записями). JWT-токен, помимо идентификатора пользователя, содержит его роль, и сервер, защищая определённые маршруты, может проверять: разрешена ли данному пользователю данная операция. Пока такого разграничения не введено (по условиям задачи не требовалось), но заложено в модель (поле role в таблице users).

**Передача данных по сети.** Взаимодействие клиент↔сервер происходит по HTTP(S). Для безопасности в рабочей эксплуатации необходимо использовать протокол **HTTPS** с валидным SSL-сертификатом. Это гарантирует шифрование всего трафика: логинов, паролей, токенов, а также передаваемых промптов и сгенерированных результатов. В среде разработки может использоваться незащищённый HTTP (http://localhost), но в бою – только HTTPS. Настройка HTTPS достигается либо за счёт reverse-proxy (например, Nginx с SSL передаёт на Uvicorn), либо используя встроенные возможности UVicorn + Hypercorn. Шифрование критически важно, чтобы злоумышленник в одной сети с пользователем не смог перехватить JWT-токен или крадущим образом прочесть содержимое передаваемых промптов.

**Защита API и данных.** Внутри серверного кода внедрены следующие меры безопасности:
- **Проверка входных данных (валидация).** Все запросы от клиента валидируются с помощью схем Pydantic (FastAPI автоматически проверит типы). Например, поле prompt должно быть строкой, не слишком длинной (можно задать ограничение длины на уровне схемы), оценка должна быть числом в диапазоне 0–100 и т.д. Это исключает случаи, когда некорректные данные приведут к непредвиденным ошибкам или, хуже, эксплойтам. Например, если бы не было валидации, злоумышленник мог попытаться отправить вместо строки промпта специальные последовательности, пытаясь вызвать сбой или уязвимость парсера. Pydantic и типизация предотвращают это, гарантируя что в бизнес-логику попадут уже проверенные значения.
- **SQL-инъекции.** Использование ORM (SQLAlchemy/Tortoise) либо подготовленных выражений исключает возможность SQL-инъекции. Пользовательский ввод никогда напрямую не подставляется в SQL-запросы – вместо этого идет либо параметризация (`cursor.execute("SELECT * FROM history WHERE user_id=%s", [user_id])`), либо высокоуровневые вызовы ORM (которые сами подставят параметры безопасно). То есть даже если злоумышленник попробует, например, в поле фильтра истории передать `1; DROP TABLE users;--`, это будет трактовано как строка параметра, а не команда, и база данных не пострадает.
- **Ограничение доступа к функциям.** Каждый защищённый эндпойнт (редактирование промпта, получение preview, просмотр истории) помечен декоратором/зависимостью `Depends(get_current_user)` – т.е. требует действующего JWT. FastAPI автоматически проверяет заголовок Authorization, валидирует токен и, если что-то не так (нет токена или подпись неверна или истёк) – возвращает ошибку 401 Unauthorized. Тем самым, гарантируется, что **только** авторизованные пользователи могут пользоваться основным функционалом. Попытка неавторизованного запроса отклоняется. 
- **Изоляция данных пользователей.** Каждый пользователь видит только свои данные (например, историю запросов). На уровне запросов к БД все выборки истории выполняются с фильтрацией по user_id, совпадающему с идентификатором текущего JWT. Таким образом, даже если кто-то попробует вручную подставить чужой ID, сервер проигнорирует это, так как возьмёт ID из токена, которому злоумышленник не может произвольно приписать другой ID (подпись JWT предотвратит). 
- **Безопасность вызова внешних API.** При обращении к Fusion Brain API используется API-ключ, выданный нашей системе. Этот ключ хранится на сервере в конфигурации (в переменных окружения или файле .env) и **никогда не передаётся клиенту**. Таким образом, пользователь платформы не узнает ключ и не сможет самостоятельно воспользоваться чужой квотой API. Все вызовы к FusionBrain идут с сервера – ключ передаётся по HTTPS, на что FusionBrain отвечает изображением. Сам ответ FusionBrain (изображение) безопасен с точки зрения выполнения (PNG или JPEG-файл не может содержать выполняемого кода для нашего сервера).
- **Контроль длины и содержимого промптов.** Поскольку пользователи вводят произвольный текст, необходимо учесть потенциальные злоупотребления. Например, очень длинный промпт (десятки тысяч символов) мог бы вызвать повышенную нагрузку или переполнение буфера. Поэтому введено ограничение: длина промпта не более 1000 символов (больше и не имеет смысла, т.к. FusionBrain ограничивает описание 1000 символами ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=%D0%92%D1%8B%20%D0%B8%20%D0%B2%D0%B0%D1%88%D0%B8%20%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D1%82%D0%B5%D0%BB%D0%B8%20%D0%BC%D0%BE%D0%B3%D1%83%D1%82,%D0%B8%D1%81%D0%BF%D0%BE%D0%BB%D1%8C%D0%B7%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D0%B5%20emoji%20%D0%B2%20%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%BE%D0%B2%D0%BE%D0%BC%20%D0%BE%D0%BF%D0%B8%D1%81%D0%B0%D0%BD%D0%B8%D0%B8))). Проверка осуществляется и на клиенте (поле ввода ограничено, можно подсвечивать количество символов), и на сервере (валидация Pydantic, обрезка). Кроме того, фильтрация по недопустимому содержимому: явно, если промпт содержит запрещённые к генерации темы (скажем, экстремизм, порнография), FusionBrain API сам может отказать. Но также и наш сервер может содержать элементарный список стоп-слов – пока не реализовано, но легко добавить. Это больше вопрос этики использования, но также и безопасности (не допустить, чтобы платформа использовалась для создания противоправного контента).
- **Предотвращение DoS на уровне приложения.** Если один пользователь злоупотребляет (например, скриптом шлёт десятки запросов в секунду), это может перегрузить либо GPU, либо вызвать превышение лимитов FusionBrain API. Чтобы этого не произошло, можно внедрить ограничитель частоты запросов (rate limit). В FastAPI нет встроенного, но можно с помощью Redis или in-memory счетчика отсекать слишком частые запросы с одного токена/IP. В рамках малой команды это неактуально, но на будущее отметим этот механизм. Тем не менее, базовая устойчивость: тяжелая операция – генерация изображения – не мгновенная, так что многократные нажатия просто встанут в очередь по сути.
- **Логи и мониторинг.** Для безопасности важно протоколировать события: попытки входа (в том числе неудачные), обращения к API. Сервер ведёт лог (например, с помощью `uvicorn.error` / `uvicorn.access`) с указанием времени, пользователя (если известен) и действия. Это поможет в случае инцидентов понять, что произошло. Конечно, логи хранятся в защищённой среде (на сервере), недоступной внешним пользователям.

**Защита данных пользователя.** Помимо паролей, которые хранятся в виде хешей, следует отметить и защиту содержимого истории промптов. Хотя они не столь конфиденциальны, все же это интеллектуальная собственность пользователя (особенно для художника: удачно сформулированный промпт – ценность). Поэтому история запросов каждого пользователя закрыта от других (как описано), а также от постороннего доступа извне (невозможно без JWT вытащить эти данные через API). При хранении на диске бэкапы БД также должны быть защищены (например, шифрованы или доступны только администратору). 

**Безопасность модели LLaMA.** Локальная модель – это код, который мы запускаем. Он загружен из надежного источника (предполагается официальные веса). Тем не менее, есть аспект: промпт, подаваемый в модель, может быть сконструирован злоумышленником так, чтобы использовать уязвимости модели. Например, *prompt injection* – когда пользователь даёт особый запрос, заставляющий модель игнорировать инструкции и, предположим, выдать скрытую информацию или выполнить чужой код. В нашем случае LLaMA 8B – модель генерации текста, она не выполняет произвольный код, потому тут скорее вопрос нежелательного контента. Если дать ей оскорбительный или опасный запрос, она может сгенерировать нежелательный текст. Чтобы это смягчить, можно внедрить **фильтр контента** на выходе модели (например, использовать модель-модератор или простой список стоп-слов для выходного текста). Так, если LLaMA вдруг выдаст что-то, нарушающее правила (нецензурную брань, персональные данные), платформе следовало бы отфильтровать. В контексте дипломного прототипа это тонкость, но для полноты стоит упомянуть: 
- Fusion Brain (Kandinsky) имеет встроенный контент-фильтр на сервере (он не создаст запрещенные изображения, а вернёт заглушку). 
- Для LLaMA можно использовать open-source модель модератора или хотя бы явно не разрешать некоторые запросы (но мы предполагаем, что наши пользователи – благонадежные разработчики/художники).

**Обновления безопасности.** Регулярно обновляется программное обеспечение: библиотеки (FastAPI, PyTorch) – это важно, так как в них могут обнаруживаться уязвимости. Использование контейнеров позволяет легко обновлять среду (например, переходить на новые образы с обновлениями безопасности OS). Также внутренний секретный ключ для JWT хранится в .env, его необходимо генерировать длинным и случайным, и при подозрении на утечку менять (это обеззопасит прошлые токены, которые станут недействительными). 

**Безопасность инфраструктуры.** Сервер, на котором размещается система, должен быть защищен: настроен файрвол (открыты только необходимые порты – 443 для HTTPS, 80 может редиректиться на 443, и, возможно, 22 для SSH админа). Доступ к серверу по SSH – только администратору с ключом, либо через VPN. PostgreSQL сервер, если отдельный, то либо локальный, либо за firewall – не в открытом интернет (или по крайней мере с ограничением по IP доступа до сервера приложения). 
Резервные копии БД следует хранить шифрованно, если хранятся вне сервера.

Подытоживая: **аутентификация** в системе реализована посредством логина/пароля с безопасным хранением (bcrypt-хеши) и выдачей JWT-токенов. **Авторизация** (проверка прав) – все основные маршруты требуют валидного токена, и пользователь имеет доступ только к своим данным. **Безопасность данных и кодов** обеспечивается на нескольких уровнях: шифрование канала (HTTPS), валидация входа, защита БД от инъекций, ограничение привилегий. Принятые меры соответствуют общепринятым практикам веб-безопасности (OWASP) и учитывают особенности нашей платформы (включая интеграцию ML). Таким образом, система защищена от большинства распространённых угроз: утечки учётных данных, несанкционированного доступа к данным, перехвата трафика, SQL-инъекций, brute-force атак на пароли и др. Это создаёт прочную основу доверия для пользователей: они могут безопасно использовать платформу, не опасаясь за сохранность своих персональных данных и уникальных промптов.

## 10. Заключение

В разделе проектирования была разработана подробная архитектура и техническое решение для платформы интерактивного формирования, оценки и предварительного просмотра запросов (промптов) к языковым и генеративным нейросетям. Платформа построена по принципу клиент-сервер и включает серверное приложение на FastAPI, взаимодействующее с локальной LLM-моделью LLaMA 8B (PyTorch) и внешним API Fusion Brain (модель Kandinsky 3.1) для генерации изображений, веб-клиент на Vue.js для удобного интерфейса и базу данных PostgreSQL для хранения пользовательских данных.

На основании требований были спроектированы конкретные методики реализации всех заявленных функций: редактирование промптов как в текстовом режиме, так и путём перетаскивания токенов; автоматизированное дополнение описаний с использованием возможностей большой языковой модели; вычисление метрики качества промпта по набору правил; преобразование формата запросов под требования разных моделей; механизм preview, позволяющий быстро получать отклики от модели (текстовые – через LLaMA, графические – через FusionBrain) и показывать их пользователю. Каждый из этих функциональных блоков был рассмотрен с точки зрения алгоритмов и программных средств, что показало реализуемость поставленных задач. 

Разработанная архитектура системы демонстрирует баланс между использованием локальных ресурсов и облачных сервисов: тяжёлая языковая модель размещена локально, что даёт независимость от внешних API и быстрый отклик, тогда как генерация изображений делегирована специализированному удалённому сервису для достижения высокого качества без чрезмерных требований к оборудованию. Клиентское приложение на Vue.js обеспечивает интерактивность и отзывчивый интерфейс, удовлетворяющий потребностям как технических, так и творческих пользователей. 

В ходе проектирования были приняты обоснованные схемотехнические, алгоритмические, программные и конструктивно-технологические решения. Выбор FastAPI и PyTorch на сервере обеспечил высокую производительность и гибкость при интеграции ML-модели, выбор Vue.js на фронтенде – удобство реализации сложного UI. Внедрение JWT-аутентификации, bcrypt-хеширования паролей и других мер безопасности позволило создать надежный механизм защиты данных, что немаловажно для приложения, работающего с пользовательским контентом. Предложены аппаратные требования для развертывания платформы: показано, что для базовой работы достаточно одной рабочей станции с GPU ~16 ГБ, а оптимальная конфигурация включает современный GPU 24 ГБ, многопроцессорный CPU и 32 ГБ RAM – что находится в пределах доступности для целевой аудитории (например, внутри организации или в облачных серверах). Также рассмотрены варианты масштабирования на несколько узлов при необходимости.

Проектирование учитывало профиль пользователей системы (разработчики и художники) – в решениях особое внимание уделено удобству интерфейса (двойной режим редактирования, визуальный просмотр результатов), а также обеспечению того, чтобы ни одна из групп не была технически ограничена в использовании платформы. Все требования заказчика по функционалу удовлетворены предлагаемым дизайном: система позволяет интерактивно экспериментировать с промптами, улучшать их качество и сразу видеть, к чему эти улучшения приводят, что в конечном итоге повышает эффективность работы с нейросетевыми моделями.

В **заключение**, спроектированная платформа представляет собой целостное решение, сочетающее современные веб-технологии и методы работы с нейросетями. Она обладает модульностью и расширяемостью – в будущем возможно подключение новых моделей (например, других языковых моделей или генератора музыки) или внедрение дополнительных функций (таких как библиотека шаблонов промптов, совместное редактирование и пр.) без пересмотра базовых принципов архитектуры. Проектировочные решения прошли проверку на соответствие требованиям: они реалистичны и подтверждены ссылками на технические характеристики используемых компонентов (например, требования VRAM для модели ([LLaMA 7B GPU Memory Requirement - Transformers - Hugging Face Forums](https://discuss.huggingface.co/t/llama-7b-gpu-memory-requirement/34323#:~:text=To%20run%20the%207B%20model,the%20model%20on%20a%20T4)), возможности API Kandinsky ([Fusion Brain — платформа для генерации изображений с помощью нейросети Кандинский](https://fusionbrain.ai/docs/doc/api-dokumentaciya/#:~:text=Fusion%20Brain%20API%20,%D0%BF%D0%BE%20API%2C%20%D1%81%D1%82%D0%B0%D0%BB%D0%B0%20%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C%20Kandinsky)), особенности фреймворков ([FastAPI - что это и зачем нужен: введение в современный веб-фреймворк](https://practicum.yandex.ru/blog/fastapi-chto-eto-i-zachem-nuzhen/#:~:text=FastAPI%20%D0%BF%D1%80%D0%B5%D0%B4%D1%81%D1%82%D0%B0%D0%B2%D0%BB%D1%8F%D0%B5%D1%82%20%D1%81%D0%BE%D0%B1%D0%BE%D0%B9%20%D1%81%D0%BE%D0%B2%D1%80%D0%B5%D0%BC%D0%B5%D0%BD%D0%BD%D1%8B%D0%B9%20%D1%84%D1%80%D0%B5%D0%B9%D0%BC%D0%B2%D0%BE%D1%80%D0%BA,%D1%81%D0%BE%D0%B7%D0%B4%D0%B0%D0%BD%D0%B8%D1%8E%20%D1%83%D1%81%D1%82%D0%BE%D0%B9%D1%87%D0%B8%D0%B2%D1%8B%D1%85%20%D0%B8%C2%A0%D0%BF%D0%BE%D0%BD%D1%8F%D1%82%D0%BD%D1%8B%D1%85%20%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%BD%D1%8B%D1%85%20%D1%80%D0%B5%D1%88%D0%B5%D0%BD%D0%B8%D0%B9)) ([Vue.js — Википедия](https://ru.wikipedia.org/wiki/Vue.js#:~:text=Vue,24%20%D0%B2%20%D1%80%D0%B5%D0%B0%D0%BA%D1%82%D0%B8%D0%B2%D0%BD%D0%BE%D0%BC%20%D1%81%D1%82%D0%B8%D0%BB%D0%B5)) и др.). Это придает уверенность, что платформа будет функционировать корректно после реализации.

Разработка данной системы внесет вклад в сферу **prompt engineering**, предоставив специалистам удобный инструмент для отладки запросов. Проектирование завершено, и следующим этапом является непосредственная реализация (кодирование) согласно изложенному дизайну, после чего платформу предстоит протестировать в условиях, приближенных к боевым, чтобы убедиться в выполнении всех требований и высоком качестве полученного решения.